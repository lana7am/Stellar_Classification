[
  {
    "objectID": "models/Decision_Tree_automl.html",
    "href": "models/Decision_Tree_automl.html",
    "title": "Decision Tree",
    "section": "",
    "text": "Code\n!pip install TPOT\n\n\nCode\nfrom tpot import TPOTClassifier\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import LabelEncoder\nfrom tpot.config import classifier_config_dict\n\n\nCode\nX_train, X_test, y_train, y_test = train_test_split(digits.data, digits.target,\n                                                    train_size=0.75, test_size=0.25, random_state=42\n\nDon’t include: * plate ID, identifies each plate in SDSS * Unique ID used for optical spectroscopic objects (this means that 2 different observations with the same spec_obj_ID must share the output class) * Field number to identify each field * Camera column to identify the scanline within the run * Rerun Number to specify how the image was processed * Run Number used to identify the specific scan * Object Identifier, the unique value that identifies the object in the image catalog used by the CAS * MJD = Modified Julian Date, used to indicate when a given piece of SDSS data was taken\n\nCode\ndf = pd.read_csv(\"star_classification.csv\")\n#just manually remove all ID-type columns, and class\nfeatures = [\n 'alpha',\n 'delta',\n 'u',\n 'g',\n 'r',\n 'i',\n 'z',\n 'redshift']\nX = df[features]\ny = df['class']\nle = LabelEncoder()\ny = le.fit_transform(y)\n\n\nCode\ntpot_config = {\n    'sklearn.tree.DecisionTreeClassifier': {\n        'criterion': [\"gini\", \"entropy\"],\n        'max_depth': range(1, 11),\n        'min_samples_split': range(2, 21),\n        'min_samples_leaf': range(1, 21)\n    }\n}\n\n\nCode\ntpot_config['tpot.builtins.FeatureSetSelector'] = {\n    'subset_list': ['https://raw.githubusercontent.com/EpistasisLab/tpot/master/tests/subset_test.csv'],\n    'sel_subset': [0,1] # select only one feature set, a list of index of subset in the list above\n    #'sel_subset': list(combinations(range(3), 2)) # select two feature sets\n}\n\n\nCode\nX_train, X_test, y_train, y_test = train_test_split(X, y,\n                                                    train_size=0.75, test_size=0.25, random_state=42)\n\n\nCode\ntpot = TPOTClassifier(config_dict=tpot_config, generations=5, population_size=50, verbosity=2, random_state=42,max_time_mins = 5)\n\n\n\nCode\ntpot.fit(X_train, y_train)\n\n\n\n\n\n\n5.01 minutes have elapsed. TPOT will close down.\nTPOT closed during evaluation in one generation.\nWARNING: TPOT may not provide a good pipeline if TPOT is stopped/interrupted in a early generation.\n\n\nTPOT closed prematurely. Will use the current best pipeline.\n\nBest pipeline: DecisionTreeClassifier(DecisionTreeClassifier(input_matrix, criterion=entropy, max_depth=10, min_samples_leaf=19, min_samples_split=18), criterion=entropy, max_depth=3, min_samples_leaf=5, min_samples_split=10)\n\n\nTPOTClassifier(config_dict={'sklearn.tree.DecisionTreeClassifier': {'criterion': ['gini',\n                                                                                  'entropy'],\n                                                                    'max_depth': range(1, 11),\n                                                                    'min_samples_leaf': range(1, 21),\n                                                                    'min_samples_split': range(2, 21)},\n                            'tpot.builtins.FeatureSetSelector': {'sel_subset': [0,\n                                                                                1],\n                                                                 'subset_list': ['https://raw.githubusercontent.com/EpistasisLab/tpot/master/tests/subset_test.csv']}},\n               generations=5, max_time_mins=5, population_size=50,\n               random_state=42, verbosity=2)\n\n\n\n\nCode\nprint(tpot.score(X_test, y_test))\n\n\n0.97568\n\n\n\nCode\ntpot.export(\"dt_pipeline.py\")\n\n\nCode\n#run pipeline\n\n\nCode\nclf = DecisionTreeClassifier(criterion=\"entropy\", max_depth=3, min_samples_leaf=5, min_samples_split=10)\n\n\nCode\nclf.fit(X_train, y_train)\ny_pred = clf.predict(X_test)\n\n\n\nCode\nprint(\"Accuracy score\", sklearn.metrics.accuracy_score(y_test, y_pred))\n\n\nAccuracy score 0.95088\n\n\n\n\nCode\nprint(classification_report(y_test, y_pred))\n\n\n              precision    recall  f1-score   support\n\n           0       0.94      0.98      0.96     14895\n           1       0.95      0.79      0.86      4769\n           2       1.00      1.00      1.00      5336\n\n    accuracy                           0.95     25000\n   macro avg       0.96      0.93      0.94     25000\nweighted avg       0.95      0.95      0.95     25000\n\n\n\n\n\nCode\nmatrix = confusion_matrix(y_pred, y_test)\nmatrix = matrix / matrix.astype(np.float).sum(axis=1)\ncm = sns.heatmap(matrix, square=True, annot=True, cbar=False,\n            xticklabels=['GALAXY', 'STAR', 'QSO'], yticklabels=['GALAXY', 'STAR', 'QSO'])\nplt.xlabel('Truth')\nplt.ylabel('Predicted')\nplt.title(\"\")\nplt.show()\n\n\n/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:2: DeprecationWarning: `np.float` is a deprecated alias for the builtin `float`. To silence this warning, use `float` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.float64` here.\nDeprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n  \n\n\n\n\n\n\nCode\n#initialize shadow tree\nsk_dtree = ShadowSKDTree(clf, X, y, features, \"class\", ['GALAXY', 'STAR', 'QSO'])\n\n\n\nCode\ntrees.viz_leaf_samples(sk_dtree)\n\n\n[WARNING] [2022-08-09 23:43:03,900:matplotlib.font_manager] findfont: Font family ['Arial'] not found. Falling back to DejaVu Sans.\n\n\n\n\n\n\n\nCode\ntrees.dtreeviz(sk_dtree)\n\n\n[WARNING] [2022-08-09 23:43:50,124:matplotlib.font_manager] findfont: Font family ['Arial'] not found. Falling back to DejaVu Sans.\n[WARNING] [2022-08-09 23:43:50,353:matplotlib.font_manager] findfont: Font family ['Arial'] not found. Falling back to DejaVu Sans.\n[WARNING] [2022-08-09 23:43:50,369:matplotlib.font_manager] findfont: Font family ['Arial'] not found. Falling back to DejaVu Sans.\n\n\n/usr/local/lib/python3.7/dist-packages/numpy/core/fromnumeric.py:3208: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n  return asarray(a).size\n/usr/local/lib/python3.7/dist-packages/matplotlib/cbook/__init__.py:1376: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n  X = np.atleast_1d(X.T if isinstance(X, np.ndarray) else np.asarray(X))\n\n\n[WARNING] [2022-08-09 23:43:51,615:matplotlib.font_manager] findfont: Font family ['Arial'] not found. Falling back to DejaVu Sans.\n\n\n\n\n\n\n\nCode\ntrees.dtreeviz(sk_dtree, fancy=False)\n\n\n\n\n\n\n\nCode\ntrees.dtreeviz(sk_dtree, show_just_path=True, X = X.iloc[10])\n\n\n/usr/local/lib/python3.7/dist-packages/numpy/core/fromnumeric.py:3208: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n  return asarray(a).size\n/usr/local/lib/python3.7/dist-packages/matplotlib/cbook/__init__.py:1376: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n  X = np.atleast_1d(X.T if isinstance(X, np.ndarray) else np.asarray(X))\n\n\n\n\n\n\n\nCode\ntrees.viz_leaf_criterion(clf)\n\n\n\n\n\n\n\nCode\ntrees.describe_node_sample(sk_dtree, node_id=10)\n\n\n\n\n  \n    \n      \n\n\n  \n    \n      \n      alpha\n      delta\n      u\n      g\n      r\n      i\n      z\n      redshift\n    \n  \n  \n    \n      count\n      51935.000000\n      51935.000000\n      51935.000000\n      51935.000000\n      51935.000000\n      51935.000000\n      51935.000000\n      51935.000000\n    \n    \n      mean\n      181.711450\n      24.074552\n      22.346448\n      20.536128\n      19.210662\n      18.515720\n      18.135271\n      0.355549\n    \n    \n      std\n      91.416751\n      19.351312\n      2.327826\n      2.027503\n      1.723310\n      1.536114\n      1.502448\n      0.203380\n    \n    \n      min\n      0.005528\n      -12.364701\n      13.897990\n      12.679020\n      11.746640\n      11.299560\n      10.897380\n      0.004285\n    \n    \n      25%\n      134.238282\n      6.283251\n      20.344025\n      18.611765\n      17.660465\n      17.226380\n      16.926995\n      0.145027\n    \n    \n      50%\n      187.270126\n      22.799320\n      22.584700\n      21.159340\n      19.592490\n      18.855040\n      18.436680\n      0.387110\n    \n    \n      75%\n      231.975632\n      38.773742\n      24.060300\n      22.144885\n      20.597755\n      19.655370\n      19.214630\n      0.534749\n    \n    \n      max\n      359.994125\n      74.459854\n      29.325650\n      29.862580\n      29.571860\n      29.889210\n      29.383740\n      0.685050\n    \n  \n\n\n      \n        \n  \n    \n    \n  \n      \n      \n  \n\n      \n    \n  \n  \n\n\n\n\nCode\npred_path = X.iloc[10]\npred_path\n\n\nalpha       328.092076\ndelta        18.220310\nu            25.771630\ng            22.520420\nr            20.638840\ni            19.780710\nz            19.057650\nredshift      0.459596\nName: 10, dtype: float64\n\n\n\n\nCode\nprint(trees.explain_prediction_path(clf, pred_path, feature_names=features, explanation_type=\"plain_english\"))\n\n\n0.0 <= redshift  < 0.69\n\n\n\n\n\nCode\ntrees.explain_prediction_path(clf, pred_path, feature_names=features, explanation_type=\"sklearn_default\")\n\n\n<matplotlib.axes._subplots.AxesSubplot at 0x7f5a40719c90>"
  },
  {
    "objectID": "models/LogisticRegression.html",
    "href": "models/LogisticRegression.html",
    "title": "LogisticRegression",
    "section": "",
    "text": "Code\n# read the dataset\ndf = pd.read_csv(\"..\\data\\star_classification.csv\")\n\n# encode values for class column\ndf.replace({'class': {'GALAXY': 0, 'STAR': 1, 'QSO':2}}, inplace=True)\n\n\n\n\nCode\nf,ax = plt.subplots(figsize=(12,8))\nsns.heatmap(df.corr(), cmap=\"PuBu\", annot=True, linewidths=0.5, fmt= '.2f',ax=ax)\nplt.show()\n\n\n\n\n\n\n\nCode\ndf.corr()[\"class\"].sort_values()\n\n\nfield_ID      -0.038044\nu             -0.017701\ng             -0.005915\nrun_ID        -0.000049\nobj_ID        -0.000047\nalpha          0.004552\ncam_col        0.014476\nz              0.017352\nfiber_ID       0.032053\ndelta          0.056643\nr              0.150691\nMJD            0.207262\nspec_obj_ID    0.215722\nplate          0.215722\ni              0.284396\nredshift       0.536822\nclass          1.000000\nrerun_ID            NaN\nName: class, dtype: float64\n\n\n\n\nCode\nfrom dis import dis\n\ncleaned = df.drop(['obj_ID','run_ID','rerun_ID',\"alpha\"], axis = 1)\n\ncleaned=cleaned.drop(79543)\n# make the X and y varialbes\nX = cleaned.drop('class', axis=1)\ny = cleaned['class']\ndisplay(cleaned)\n\n\nsm = SMOTE(random_state=42)\nX, y = sm.fit_resample(X, y)\n\nX_train, X_test, y_train, y_test = train_test_split(X, y, train_size=0.7, random_state=123)\ncleaned.var()\n\n\n\n\n\n\n  \n    \n      \n      alpha\n      delta\n      u\n      g\n      r\n      i\n      z\n      cam_col\n      field_ID\n      spec_obj_ID\n      class\n      redshift\n      plate\n      MJD\n      fiber_ID\n    \n  \n  \n    \n      0\n      135.689107\n      32.494632\n      23.87882\n      22.27530\n      20.39501\n      19.16573\n      18.79371\n      2\n      79\n      6.543777e+18\n      0\n      0.634794\n      5812\n      56354\n      171\n    \n    \n      1\n      144.826101\n      31.274185\n      24.77759\n      22.83188\n      22.58444\n      21.16812\n      21.61427\n      5\n      119\n      1.176014e+19\n      0\n      0.779136\n      10445\n      58158\n      427\n    \n    \n      2\n      142.188790\n      35.582444\n      25.26307\n      22.66389\n      20.60976\n      19.34857\n      18.94827\n      2\n      120\n      5.152200e+18\n      0\n      0.644195\n      4576\n      55592\n      299\n    \n    \n      3\n      338.741038\n      -0.402828\n      22.13682\n      23.77656\n      21.61162\n      20.50454\n      19.25010\n      3\n      214\n      1.030107e+19\n      0\n      0.932346\n      9149\n      58039\n      775\n    \n    \n      4\n      345.282593\n      21.183866\n      19.43718\n      17.58028\n      16.49747\n      15.97711\n      15.54461\n      3\n      137\n      6.891865e+18\n      0\n      0.116123\n      6121\n      56187\n      842\n    \n    \n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n    \n    \n      99995\n      39.620709\n      -2.594074\n      22.16759\n      22.97586\n      21.90404\n      21.30548\n      20.73569\n      2\n      581\n      1.055431e+19\n      0\n      0.000000\n      9374\n      57749\n      438\n    \n    \n      99996\n      29.493819\n      19.798874\n      22.69118\n      22.38628\n      20.45003\n      19.75759\n      19.41526\n      1\n      289\n      8.586351e+18\n      0\n      0.404895\n      7626\n      56934\n      866\n    \n    \n      99997\n      224.587407\n      15.700707\n      21.16916\n      19.26997\n      18.20428\n      17.69034\n      17.35221\n      4\n      308\n      3.112008e+18\n      0\n      0.143366\n      2764\n      54535\n      74\n    \n    \n      99998\n      212.268621\n      46.660365\n      25.35039\n      21.63757\n      19.91386\n      19.07254\n      18.62482\n      4\n      131\n      7.601080e+18\n      0\n      0.455040\n      6751\n      56368\n      470\n    \n    \n      99999\n      196.896053\n      49.464643\n      22.62171\n      21.79745\n      20.60115\n      20.00959\n      19.28075\n      4\n      60\n      8.343152e+18\n      0\n      0.542944\n      7410\n      57104\n      851\n    \n  \n\n99999 rows × 15 columns\n\n\n\nalpha          9.312754e+03\ndelta          3.859106e+02\nu              5.067309e+00\ng              4.150934e+00\nr              3.440146e+00\ni              3.090214e+00\nz              3.118692e+00\ncam_col        2.518293e+00\nfield_ID       2.220329e+04\nspec_obj_ID    1.104915e+37\nclass          6.201753e-01\nredshift       5.339351e-01\nplate          8.716149e+06\nMJD            3.270644e+06\nfiber_ID       7.425530e+04\ndtype: float64\n\n\n\nCode\nsns.displot(df[\"run_ID\"]);\n\n\nCode\nsns.displot(df[\"field_ID\"]);\n\n\nCode\nsns.displot(df[\"MJD\"]);\n\n\nCode\ncleaned[cleaned[\"redshift\"]>5]\n\n\nCode\nsns.displot(cleaned[\"alpha\"]);\n\n\nCode\nsns.displot(cleaned[\"delta\"]);\n\n\nCode\nsns.displot(x=cleaned[\"u\"]);\n\n\nCode\ncleaned\n\n\nCode\nsns.displot(x=cleaned[\"g\"]);\n\n\nCode\nsns.displot(cleaned[\"r\"]);\n\n\nCode\nsns.displot(cleaned[\"i\"]);\n\n\nCode\nsns.displot(cleaned[\"z\"]);\n\n\nCode\nsns.countplot(x=cleaned[\"cam_col\"]);\n\n\nCode\nsns.countplot(x=cleaned[\"class\"]);\n\n\nCode\nsns.displot(cleaned[\"redshift\"]);\n\n\nCode\nsns.displot(cleaned[\"plate\"]);\n\n\npre-processing\n\nCode\n# Normalizing approach\nyj = PowerTransformer(method=\"yeo-johnson\")\nscaler = StandardScaler()\n# nzv_encoder = VarianceThreshold(threshold=0.1)\n# pca = PCA(n_components=7)\n\n# Normalize all numeric features\npreprocessor = ColumnTransformer([(\"norm\", yj, selector(dtype_include=\"number\")),\n                (\"std_encode\", scaler, selector(dtype_include=\"number\")),\n                # (\"nzv_encoder\", nzv_encoder, selector(dtype_include=\"number\")),\n                # (\"pca_encode\", pca, selector(dtype_include=\"number\"))\n                ])\n\n\n\nRandom Forest Classifier\n\n\nCode\n\n#best training data for Random Forest Classifier\ntree_X_train= X_train[['delta', 'u', 'g', 'i',\"spec_obj_ID\", 'redshift', 'plate']]\ntree_X_test= X_test[['delta', 'u', 'g', 'i',\"spec_obj_ID\", 'redshift', 'plate']]\n\n\n#creating the Random Forest Classifier model with the pre processing steps\nr_forest = RandomForestClassifier()\nr_forest_pipeline = Pipeline(steps=[\n  (\"norm\", yj),\n  (\"std_encode\",scaler),\n  (\"r_forest\", r_forest),\n])\n\n\n# training the model\nr_forest_pipeline.fit(tree_X_train,y_train)\npredicted = r_forest_pipeline.predict(tree_X_test)\nscore = r_forest_pipeline.score(tree_X_test,y_test)\nr_forest_pipeline_score = np.mean(score)\nr_forest_pipeline_score\n\n# 0.983\n\n\nc:\\Users\\user\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\numpy\\core\\_methods.py:236: RuntimeWarning: overflow encountered in multiply\n\n\n0.9824676174277116\n\n\n\nCode\n# define loss function\nscoring = 'accuracy'\n\n# create 10 fold CV object\nkfold = KFold(n_splits=5, random_state=123, shuffle=True)\n\n# # fit model with 10-fold CV\nresults = cross_val_score(r_forest_pipeline, X_test, y_test, cv=kfold, scoring=scoring)\nresults.mean()\n\n\nCode\n#feature selection\ntsfs=SFS(r_forest_pipeline,k_features=14,scoring=scoring,cv=kfold)\ntsfs.fit(X,y)\ntsfs.subsets_\n\n\n#best output\n#  7: {'feature_idx': (2, 3, 4, 6, 12, 13, 14),\n#   'cv_scores': array([0.98320576, 0.98357025, 0.9836824 , 0.98457958, 0.98340202]),\n#   'avg_score': 0.9836880029158606,\n#   'feature_names': ('delta','u','g','i','spec_obj_ID','redshift','plate')},\n\n\nCode\n# Create grid of hyperparameter values\n\n# Number of trees in random forest\nn_estimators = [int(x) for x in np.linspace(start = 200, stop = 2000, num = 10)]\n# Number of features to consider at every split\n# Maximum number of levels in tree\nmax_depth = [int(x) for x in np.linspace(10, 110, num = 11)]\nmax_depth.append(None)\n# Minimum number of samples required to split a node\nmin_samples_split = [2, 5, 10]\n# Minimum number of samples required at each leaf node\nmin_samples_leaf = [1, 2, 4]\n# Method of selecting samples for training each tree\nbootstrap = [True, False]\n\nhyper_grid = {'r_forest__n_estimators': n_estimators,\n               'r_forest__max_depth': max_depth,\n               'r_forest__min_samples_split': min_samples_split,\n               'r_forest__min_samples_leaf': min_samples_leaf,\n               'r_forest__bootstrap': bootstrap}\n\n\ngrid_search = GridSearchCV(r_forest_pipeline, hyper_grid, cv=kfold, scoring=scoring)\nresults = grid_search.fit(X_train[[\"g\",\"i\",\"redshift\"]], y_train)\n\n\n\nSVM\n\n\nCode\n\n#best training data for Random Forest Classifier\n#??\n\n\n#creating SVM model\nsvm_clf = svm.SVC(kernel='rbf', C=2, random_state=0)\nmodel_pipeline = Pipeline(steps=[\n  (\"norm\", yj),\n  (\"std_encode\",scaler),\n  (\"knn\", svm_clf),\n])\n\n\n#training SVM model\n# model_pipeline.fit(X_train,y_train)\n# predicted = model_pipeline.predict(X_test)\n# score = model_pipeline.score(X_test,y_test)\n# model_pipeline_score = np.mean(score)\n# model_pipeline_score\n#0.977\n\n\n0.973589278705071\n\n\n\nCode\n# define loss function\nscoring = 'accuracy'\n\n# create 10 fold CV object\nkfold = KFold(n_splits=5, random_state=123, shuffle=True)\n\n# # fit model with 10-fold CV\n# results = cross_val_score(model_pipeline, X_test, y_test, cv=kfold, scoring=scoring)\n# results.mean()\n\n#feature selection\nsvmsfs=SFS(model_pipeline,\n    k_features=17,\n    scoring=scoring,\n    cv=kfold)\nsvmsfs.fit(X,y)\nsvmsfs.subsets_\n\n\n\nLogistic Regression\n\n\nCode\n#best training data for Logistic Regression\nlog_X_train=X_train[[\"delta\",\"u\",\"g\",\"r\",\"cam_col\",\"field_ID\",\"spec_obj_ID\",\"redshift\",\"plate\",\"MJD\",\"fiber_ID\"]]\nlog_X_test=X_test[[\"delta\",\"u\",\"g\",\"r\",\"cam_col\",\"field_ID\",\"spec_obj_ID\",\"redshift\",\"plate\",\"MJD\",\"fiber_ID\"]]\n\n#creating Logistic Regression model\nlog_reg=LogisticRegression(max_iter=1000,C=4714.85,penalty=\"l2\")\nlog_reg_pipeline = Pipeline(steps=[\n  (\"norm\", yj),\n  (\"std_encode\",scaler),\n  (\"log_reg\", log_reg),\n])\n\n#training Logistic Regression model\nlog_reg_pipeline.fit(X_train,y_train)\npredicted = log_reg_pipeline.predict(X_test)\nscore = log_reg_pipeline.score(X_test,y_test)\nlog_reg_pipeline = np.mean(score)\nlog_reg_pipeline\n\n\nc:\\Users\\user\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\numpy\\core\\_methods.py:236: RuntimeWarning: overflow encountered in multiply\n\n\n0.9692902936393712\n\n\n\n\nCode\n# define scoring function\nscoring = 'accuracy'\n\n# create 10 fold CV object\nkfold = KFold(n_splits=5, random_state=123, shuffle=True)\n\n#  fit model with 10-fold CV\nresults = cross_val_score(log_reg_pipeline, X_test, y_test, cv=kfold, scoring=scoring)\nresults.mean()\n\n\n\nc:\\Users\\user\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\numpy\\core\\_methods.py:236: RuntimeWarning: overflow encountered in multiply\nc:\\Users\\user\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\numpy\\core\\_methods.py:236: RuntimeWarning: overflow encountered in multiply\nc:\\Users\\user\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\numpy\\core\\_methods.py:236: RuntimeWarning: overflow encountered in multiply\nc:\\Users\\user\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\numpy\\core\\_methods.py:236: RuntimeWarning: overflow encountered in multiply\nc:\\Users\\user\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\numpy\\core\\_methods.py:236: RuntimeWarning: overflow encountered in multiply\nc:\\Users\\user\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\numpy\\core\\_methods.py:236: RuntimeWarning: overflow encountered in multiply\nc:\\Users\\user\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\numpy\\core\\_methods.py:236: RuntimeWarning: overflow encountered in multiply\nc:\\Users\\user\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\numpy\\core\\_methods.py:236: RuntimeWarning: overflow encountered in multiply\nc:\\Users\\user\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\numpy\\core\\_methods.py:236: RuntimeWarning: overflow encountered in multiply\nc:\\Users\\user\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\numpy\\core\\_methods.py:236: RuntimeWarning: overflow encountered in multiply\nc:\\Users\\user\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\numpy\\core\\_methods.py:236: RuntimeWarning: overflow encountered in multiply\nc:\\Users\\user\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\numpy\\core\\_methods.py:236: RuntimeWarning: overflow encountered in multiply\nc:\\Users\\user\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\numpy\\core\\_methods.py:236: RuntimeWarning: overflow encountered in multiply\nc:\\Users\\user\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\numpy\\core\\_methods.py:236: RuntimeWarning: overflow encountered in multiply\nc:\\Users\\user\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\numpy\\core\\_methods.py:236: RuntimeWarning: overflow encountered in multiply\nc:\\Users\\user\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\numpy\\core\\_methods.py:236: RuntimeWarning: overflow encountered in multiply\nc:\\Users\\user\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\numpy\\core\\_methods.py:236: RuntimeWarning: overflow encountered in multiply\nc:\\Users\\user\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\numpy\\core\\_methods.py:236: RuntimeWarning: overflow encountered in multiply\nc:\\Users\\user\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\numpy\\core\\_methods.py:236: RuntimeWarning: overflow encountered in multiply\nc:\\Users\\user\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\numpy\\core\\_methods.py:236: RuntimeWarning: overflow encountered in multiply\nc:\\Users\\user\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\numpy\\core\\_methods.py:236: RuntimeWarning: overflow encountered in multiply\nc:\\Users\\user\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\numpy\\core\\_methods.py:236: RuntimeWarning: overflow encountered in multiply\nc:\\Users\\user\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\numpy\\core\\_methods.py:236: RuntimeWarning: overflow encountered in multiply\nc:\\Users\\user\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\numpy\\core\\_methods.py:236: RuntimeWarning: overflow encountered in multiply\nc:\\Users\\user\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\numpy\\core\\_methods.py:236: RuntimeWarning: overflow encountered in multiply\nc:\\Users\\user\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\numpy\\core\\_methods.py:236: RuntimeWarning: overflow encountered in multiply\nc:\\Users\\user\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\numpy\\core\\_methods.py:236: RuntimeWarning: overflow encountered in multiply\nc:\\Users\\user\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\numpy\\core\\_methods.py:236: RuntimeWarning: overflow encountered in multiply\nc:\\Users\\user\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\numpy\\core\\_methods.py:236: RuntimeWarning: overflow encountered in multiply\nc:\\Users\\user\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\numpy\\core\\_methods.py:236: RuntimeWarning: overflow encountered in multiply\nc:\\Users\\user\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\numpy\\core\\_methods.py:236: RuntimeWarning: overflow encountered in multiply\nc:\\Users\\user\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\numpy\\core\\_methods.py:236: RuntimeWarning: overflow encountered in multiply\nc:\\Users\\user\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\numpy\\core\\_methods.py:236: RuntimeWarning: overflow encountered in multiply\nc:\\Users\\user\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\numpy\\core\\_methods.py:236: RuntimeWarning: overflow encountered in multiply\nc:\\Users\\user\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\numpy\\core\\_methods.py:236: RuntimeWarning: overflow encountered in multiply\nc:\\Users\\user\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\numpy\\core\\_methods.py:236: RuntimeWarning: overflow encountered in multiply\nc:\\Users\\user\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\numpy\\core\\_methods.py:236: RuntimeWarning: overflow encountered in multiply\nc:\\Users\\user\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\numpy\\core\\_methods.py:236: RuntimeWarning: overflow encountered in multiply\nc:\\Users\\user\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\numpy\\core\\_methods.py:236: RuntimeWarning: overflow encountered in multiply\nc:\\Users\\user\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\numpy\\core\\_methods.py:236: RuntimeWarning: overflow encountered in multiply\nc:\\Users\\user\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\numpy\\core\\_methods.py:236: RuntimeWarning: overflow encountered in multiply\nc:\\Users\\user\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\numpy\\core\\_methods.py:236: RuntimeWarning: overflow encountered in multiply\nc:\\Users\\user\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\numpy\\core\\_methods.py:236: RuntimeWarning: overflow encountered in multiply\nc:\\Users\\user\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\numpy\\core\\_methods.py:236: RuntimeWarning: overflow encountered in multiply\nc:\\Users\\user\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\numpy\\core\\_methods.py:236: RuntimeWarning: overflow encountered in multiply\nc:\\Users\\user\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\numpy\\core\\_methods.py:236: RuntimeWarning: overflow encountered in multiply\nc:\\Users\\user\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\numpy\\core\\_methods.py:236: RuntimeWarning: overflow encountered in multiply\nc:\\Users\\user\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\numpy\\core\\_methods.py:236: RuntimeWarning: overflow encountered in multiply\nc:\\Users\\user\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\numpy\\core\\_methods.py:236: RuntimeWarning: overflow encountered in multiply\nc:\\Users\\user\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\numpy\\core\\_methods.py:236: RuntimeWarning: overflow encountered in multiply\nc:\\Users\\user\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\numpy\\core\\_methods.py:236: RuntimeWarning: overflow encountered in multiply\nc:\\Users\\user\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\numpy\\core\\_methods.py:236: RuntimeWarning: overflow encountered in multiply\nc:\\Users\\user\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\numpy\\core\\_methods.py:236: RuntimeWarning: overflow encountered in multiply\nc:\\Users\\user\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\numpy\\core\\_methods.py:236: RuntimeWarning: overflow encountered in multiply\nc:\\Users\\user\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\numpy\\core\\_methods.py:236: RuntimeWarning: overflow encountered in multiply\nc:\\Users\\user\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\numpy\\core\\_methods.py:236: RuntimeWarning: overflow encountered in multiply\nc:\\Users\\user\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\numpy\\core\\_methods.py:236: RuntimeWarning: overflow encountered in multiply\nc:\\Users\\user\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\numpy\\core\\_methods.py:236: RuntimeWarning: overflow encountered in multiply\nc:\\Users\\user\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\numpy\\core\\_methods.py:236: RuntimeWarning: overflow encountered in multiply\nc:\\Users\\user\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\numpy\\core\\_methods.py:236: RuntimeWarning: overflow encountered in multiply\nc:\\Users\\user\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\numpy\\core\\_methods.py:236: RuntimeWarning: overflow encountered in multiply\nc:\\Users\\user\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\numpy\\core\\_methods.py:236: RuntimeWarning: overflow encountered in multiply\nc:\\Users\\user\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\numpy\\core\\_methods.py:236: RuntimeWarning: overflow encountered in multiply\nc:\\Users\\user\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\numpy\\core\\_methods.py:236: RuntimeWarning: overflow encountered in multiply\nc:\\Users\\user\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\numpy\\core\\_methods.py:236: RuntimeWarning: overflow encountered in multiply\nc:\\Users\\user\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\numpy\\core\\_methods.py:236: RuntimeWarning: overflow encountered in multiply\nc:\\Users\\user\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\numpy\\core\\_methods.py:236: RuntimeWarning: overflow encountered in multiply\nc:\\Users\\user\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\numpy\\core\\_methods.py:236: RuntimeWarning: overflow encountered in multiply\nc:\\Users\\user\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\numpy\\core\\_methods.py:236: RuntimeWarning: overflow encountered in multiply\nc:\\Users\\user\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\numpy\\core\\_methods.py:236: RuntimeWarning: overflow encountered in multiply\nc:\\Users\\user\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\numpy\\core\\_methods.py:236: RuntimeWarning: overflow encountered in multiply\nc:\\Users\\user\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\numpy\\core\\_methods.py:236: RuntimeWarning: overflow encountered in multiply\nc:\\Users\\user\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\numpy\\core\\_methods.py:236: RuntimeWarning: overflow encountered in multiply\nc:\\Users\\user\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\numpy\\core\\_methods.py:236: RuntimeWarning: overflow encountered in multiply\nc:\\Users\\user\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\numpy\\core\\_methods.py:236: RuntimeWarning: overflow encountered in multiply\nc:\\Users\\user\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\numpy\\core\\_methods.py:236: RuntimeWarning: overflow encountered in multiply\nc:\\Users\\user\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\numpy\\core\\_methods.py:236: RuntimeWarning: overflow encountered in multiply\nc:\\Users\\user\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\numpy\\core\\_methods.py:236: RuntimeWarning: overflow encountered in multiply\nc:\\Users\\user\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\numpy\\core\\_methods.py:236: RuntimeWarning: overflow encountered in multiply\nc:\\Users\\user\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\numpy\\core\\_methods.py:236: RuntimeWarning: overflow encountered in multiply\nc:\\Users\\user\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\numpy\\core\\_methods.py:236: RuntimeWarning: overflow encountered in multiply\nc:\\Users\\user\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\numpy\\core\\_methods.py:236: RuntimeWarning: overflow encountered in multiply\nc:\\Users\\user\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\numpy\\core\\_methods.py:236: RuntimeWarning: overflow encountered in multiply\nc:\\Users\\user\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\numpy\\core\\_methods.py:236: RuntimeWarning: overflow encountered in multiply\nc:\\Users\\user\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\numpy\\core\\_methods.py:236: RuntimeWarning: overflow encountered in multiply\nc:\\Users\\user\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\numpy\\core\\_methods.py:236: RuntimeWarning: overflow encountered in multiply\nc:\\Users\\user\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\numpy\\core\\_methods.py:236: RuntimeWarning: overflow encountered in multiply\nc:\\Users\\user\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\numpy\\core\\_methods.py:236: RuntimeWarning: overflow encountered in multiply\nc:\\Users\\user\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\numpy\\core\\_methods.py:236: RuntimeWarning: overflow encountered in multiply\nc:\\Users\\user\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\numpy\\core\\_methods.py:236: RuntimeWarning: overflow encountered in multiply\nc:\\Users\\user\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:444: ConvergenceWarning: lbfgs failed to converge (status=1):\nSTOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n\nIncrease the number of iterations (max_iter) or scale the data as shown in:\n    https://scikit-learn.org/stable/modules/preprocessing.html\nPlease also refer to the documentation for alternative solver options:\n    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\nc:\\Users\\user\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\numpy\\core\\_methods.py:236: RuntimeWarning: overflow encountered in multiply\nc:\\Users\\user\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\numpy\\core\\_methods.py:236: RuntimeWarning: overflow encountered in multiply\nc:\\Users\\user\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\numpy\\core\\_methods.py:236: RuntimeWarning: overflow encountered in multiply\nc:\\Users\\user\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\numpy\\core\\_methods.py:236: RuntimeWarning: overflow encountered in multiply\nc:\\Users\\user\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:444: ConvergenceWarning: lbfgs failed to converge (status=1):\nSTOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n\nIncrease the number of iterations (max_iter) or scale the data as shown in:\n    https://scikit-learn.org/stable/modules/preprocessing.html\nPlease also refer to the documentation for alternative solver options:\n    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\nc:\\Users\\user\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\numpy\\core\\_methods.py:236: RuntimeWarning: overflow encountered in multiply\nc:\\Users\\user\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\numpy\\core\\_methods.py:236: RuntimeWarning: overflow encountered in multiply\nc:\\Users\\user\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\numpy\\core\\_methods.py:236: RuntimeWarning: overflow encountered in multiply\nc:\\Users\\user\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\numpy\\core\\_methods.py:236: RuntimeWarning: overflow encountered in multiply\nc:\\Users\\user\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:444: ConvergenceWarning: lbfgs failed to converge (status=1):\nSTOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n\nIncrease the number of iterations (max_iter) or scale the data as shown in:\n    https://scikit-learn.org/stable/modules/preprocessing.html\nPlease also refer to the documentation for alternative solver options:\n    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\nc:\\Users\\user\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\numpy\\core\\_methods.py:236: RuntimeWarning: overflow encountered in multiply\n\nSTOPPING EARLY DUE TO KEYBOARD INTERRUPT...\n\n\n{1: {'feature_idx': (10,),\n  'cv_scores': array([0.93876693, 0.94067345, 0.93848656, 0.94286035, 0.94117812]),\n  'avg_score': 0.9403930804385006,\n  'feature_names': ('redshift',)},\n 2: {'feature_idx': (3, 10),\n  'cv_scores': array([0.96840216, 0.96871057, 0.9678975 , 0.96966383, 0.96890683]),\n  'avg_score': 0.9687161802226146,\n  'feature_names': ('g', 'redshift')},\n 3: {'feature_idx': (1, 3, 10),\n  'cv_scores': array([0.96798161, 0.96915917, 0.96829002, 0.97030869, 0.96904702]),\n  'avg_score': 0.968957299464491,\n  'feature_names': ('delta', 'g', 'redshift')},\n 4: {'feature_idx': (0, 1, 3, 10),\n  'cv_scores': array([0.96837413, 0.96915917, 0.96848628, 0.97050495, 0.9691872 ]),\n  'avg_score': 0.9691423444640705,\n  'feature_names': ('alpha', 'delta', 'g', 'redshift')},\n 5: {'feature_idx': (0, 1, 3, 10, 13),\n  'cv_scores': array([0.96829002, 0.9691872 , 0.96868254, 0.97047691, 0.96913113]),\n  'avg_score': 0.9691535593125298,\n  'feature_names': ('alpha', 'delta', 'g', 'redshift', 'fiber_ID')},\n 6: {'feature_idx': (0, 1, 3, 7, 10, 13),\n  'cv_scores': array([0.9678975 , 0.96932739, 0.96885076, 0.97025261, 0.96946758]),\n  'avg_score': 0.9691591667367595,\n  'feature_names': ('alpha', 'delta', 'g', 'cam_col', 'redshift', 'fiber_ID')},\n 7: {'feature_idx': (0, 1, 3, 7, 8, 10, 13),\n  'cv_scores': array([0.96792553, 0.9694115 , 0.96882272, 0.97028065, 0.96943954]),\n  'avg_score': 0.9691759890094485,\n  'feature_names': ('alpha',\n   'delta',\n   'g',\n   'cam_col',\n   'field_ID',\n   'redshift',\n   'fiber_ID')},\n 8: {'feature_idx': (0, 1, 3, 7, 8, 10, 11, 13),\n  'cv_scores': array([0.9676732 , 0.9691872 , 0.96848628, 0.9703928 , 0.96921524]),\n  'avg_score': 0.9689909440098692,\n  'feature_names': ('alpha',\n   'delta',\n   'g',\n   'cam_col',\n   'field_ID',\n   'redshift',\n   'plate',\n   'fiber_ID')},\n 9: {'feature_idx': (0, 1, 3, 7, 8, 10, 11, 12, 13),\n  'cv_scores': array([0.96761713, 0.96938346, 0.96854235, 0.97025261, 0.96949561]),\n  'avg_score': 0.9690582331006252,\n  'feature_names': ('alpha',\n   'delta',\n   'g',\n   'cam_col',\n   'field_ID',\n   'redshift',\n   'plate',\n   'MJD',\n   'fiber_ID')},\n 10: {'feature_idx': (0, 1, 3, 7, 8, 9, 10, 11, 12, 13),\n  'cv_scores': array([0.96761713, 0.96938346, 0.96854235, 0.97025261, 0.96952365]),\n  'avg_score': 0.969063840524855,\n  'feature_names': ('alpha',\n   'delta',\n   'g',\n   'cam_col',\n   'field_ID',\n   'spec_obj_ID',\n   'redshift',\n   'plate',\n   'MJD',\n   'fiber_ID')},\n 11: {'feature_idx': (0, 1, 2, 3, 7, 8, 9, 10, 11, 12, 13),\n  'cv_scores': array([0.96761713, 0.96927132, 0.96834609, 0.97005635, 0.96971991]),\n  'avg_score': 0.9690021588583285,\n  'feature_names': ('alpha',\n   'delta',\n   'u',\n   'g',\n   'cam_col',\n   'field_ID',\n   'spec_obj_ID',\n   'redshift',\n   'plate',\n   'MJD',\n   'fiber_ID')},\n 12: {'feature_idx': (0, 1, 2, 3, 5, 7, 8, 9, 10, 11, 12, 13),\n  'cv_scores': array([0.96781338, 0.96929935, 0.96756105, 0.97081336, 0.96901898]),\n  'avg_score': 0.9689012252221942,\n  'feature_names': ('alpha',\n   'delta',\n   'u',\n   'g',\n   'i',\n   'cam_col',\n   'field_ID',\n   'spec_obj_ID',\n   'redshift',\n   'plate',\n   'MJD',\n   'fiber_ID')},\n 13: {'feature_idx': (0, 1, 2, 3, 4, 5, 7, 8, 9, 10, 11, 12, 13),\n  'cv_scores': array([0.96859842, 0.96991617, 0.96851431, 0.97128999, 0.9696358 ]),\n  'avg_score': 0.9695909384024448,\n  'feature_names': ('alpha',\n   'delta',\n   'u',\n   'g',\n   'r',\n   'i',\n   'cam_col',\n   'field_ID',\n   'spec_obj_ID',\n   'redshift',\n   'plate',\n   'MJD',\n   'fiber_ID')}}\n\n\n\nCode\n# Create grid of hyperparameter values\nC = np.logspace(-4, 4, 50)\npenalty = ['l1', 'l2']\n\nhyper_grid = {'log_reg__C': C,\n        'log_reg__penalty':penalty,\n        }\n\n# Tune a Logistic Regression model using grid search\ngrid_search = GridSearchCV(model_pipeline, hyper_grid, cv=kfold, scoring=scoring)\nresults = grid_search.fit(X_train, y_train)\n\nresults.best_params_\n#best output\n#C=4714.85\n#penalty=\"l2\"\n\n\nCode\n#feature selection\nsfs=SFS(log_reg_pipeline,\n    k_features=14,\n    scoring=scoring,\n    cv=kfold)\n    \nsfs.fit(X,y)\nsfs.subsets_\n\n\n#best output\n#  13: {'feature_idx': ( 1, 2, 3, 4, 5, 7, 8, 9, 10, 11, 12, 13),\n#   'cv_scores': array([0.96859842, 0.96991617, 0.96851431, 0.97128999, 0.9696358 ]),\n#   'avg_score': 0.9695909384024448,\n#   'feature_names': ('delta','u','g','r','i','cam_col','field_ID','spec_obj_ID','redshift','plate','MJD','fiber_ID')}}\n\n\n\nK-Nearest Neighbors\n\n\nCode\n#best training data for K-Nearest Neighbors\nlog_X_train=X_train[['g', 'r', 'i', 'z', 'redshift', 'plate']]\nlog_X_test=X_test[['g', 'r', 'i', 'z', 'redshift', 'plate']]\n\n\n#creating the Knn model\nknn=KNeighborsClassifier(n_neighbors=3)\nknn_pipeline = Pipeline(steps=[\n  (\"norm\", yj),\n  (\"std_encode\",scaler),\n  (\"knn\", knn),\n])\n\n#training the KNN model\nknn_pipeline.fit(X_train,y_train)\npredicted = knn_pipeline.predict(X_test)\nscore = knn_pipeline.score(X_test,y_test)\nknn_pipeline_score = np.mean(score)\nknn_pipeline_score\n\n\nc:\\Users\\user\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\numpy\\core\\_methods.py:236: RuntimeWarning: overflow encountered in multiply\n\n\n0.9580381675108877\n\n\n\nCode\n# define loss function\nscoring = 'accuracy'\n\n# create 10 fold CV object\nkfold = KFold(n_splits=5, random_state=123, shuffle=True)\n\n# # fit model with 10-fold CV\nresults = cross_val_score(model_pipeline, X_test, y_test, cv=kfold, scoring=scoring)\nresults.mean()\n\n\n\nCode\n# hyper parameters for CV\nhyper_params = {\n    'n_neighbors': range(1, 10+1)\n}\n\n# Tune `knn` using grid search\ngrid_search = GridSearchCV(knn_pipeline, hyper_params, cv=kfold, scoring='accuracy')\ngrid_results = grid_search.fit(log_X_train, y_train)\n\n#best output\n# 3\n\n\nCode\n#feature selection\nsfs=SFS(knn_pipeline,\n    k_features=13,\n    scoring=scoring,\n    cv=kfold)\n    \nsfs.fit(X,y)\nsfs.subsets_\n\n#best output\n#  {'feature_idx': (4, 5, 6, 7, 13, 14),\n#   'cv_scores': array([0.97300025, 0.97330866, 0.97364511, 0.975075  , 0.97473855]),\n#   'avg_score': 0.9739535144531359,\n#   'feature_names': ('g', 'r', 'i', 'z', 'redshift', 'plate')}, \n\n\n\ndeep learining\n\nCode\nfrom dis import dis\n\n# cleaned = df.drop(['obj_ID','alpha','delta','run_ID','rerun_ID','cam_col','field_ID','fiber_ID'], axis = 1)\ncleaned = df.drop(['u','r','i','z','obj_ID','spec_obj_ID','MJD'], axis = 1)\n\n# cleaned = df.drop(df.filter(regex='ID$').columns, axis=1)\n# drop the date column\n# cleaned = cleaned.drop([\"MJD\",\"plate\",\"cam_col\"], axis=1)\ncleaned=cleaned.drop(79543)\n# make the X and y varialbes\nX = cleaned.drop('class', axis=1)\ny = cleaned['class']\ndisplay(cleaned)\n\n\nsm = SMOTE(random_state=42)\nX, y = sm.fit_resample(X, y)\n\n\nX_train, X_test, y_train, y_test = train_test_split(X, y, train_size=0.7, random_state=123)\n\nfrom tensorflow.keras import utils\n# y = utils.to_categorical(y)\n# y_train = utils.to_categorical(y_train)\n# y_test = utils.to_categorical(y_test)\n\n\nCode\n\n# define the keras model\nmodel = Sequential()\nmodel.add(Dense(units=64, input_dim=20, activation=\"tanh\"))\nmodel.add(Dense(units=64,  activation=\"tanh\"))\nmodel.add(Dense(units=32,  activation=\"tanh\"))\nmodel.add(Dense(units=3, activation='softmax'))\n\n# compile the keras model\nmodel.compile(\n    loss='sparse_categorical_crossentropy', \n    optimizer=\"rmsprop\",\n    metrics='accuracy'\n)\n# fit the model\n# model.fit(X, y, epochs=20, validation_split=0.2)\nmodel_pipeline = Pipeline(steps=[\n  (\"preprocessor\", preprocessor),\n  (\"model\", model),\n])\nm1=model_pipeline.fit(X_train,y_train, model__epochs=20, model__validation_split=0.2,model__batch_size=32,)\n\n\nCode\npredicted=m1.predict(X_test)\ny_classes = predicted.argmax(axis=-1)\n\n# model_pipeline.transform(X_test)\nprint(classification_report(y_test, y_classes)) \n\n\nCode\n# define loss function\nloss = 'accuracy'\n\n# create 10 fold CV object\nkfold = KFold(n_splits=5, random_state=123, shuffle=True)\n\n# fit model with 10-fold CV\nresults = cross_val_score(model_pipeline, X_train, y_train, cv=kfold, scoring=loss)\nresults\n\n\nCode\n# hyper_grid = {'n_neighbors': range(2, 26)}\n# grid_search = GridSearchCV(knn, hyper_grid, cv=kfold, scoring=loss)\n# results = grid_search.fit(X_train, y_train)\n\n\n\nk-means clustring | trash\n\nCode\nmodel = sklearn.cluster.KMeans(n_clusters=3,random_state=123)\nmodel\n\n\nCode\ngalaxy = cleaned[cleaned[\"class\"]==0].drop(\"class\",axis=1)\ngalaxy_centers = map(lambda a: a/galaxy.shape[0],galaxy.sum())\ngalaxy_centers= np.array(list(galaxy_centers))\ngalaxy_centers\n\n\nSTAR = cleaned[cleaned[\"class\"]==1].drop(\"class\",axis=1)\nSTAR_centers = map(lambda a: a/STAR.shape[0],STAR.sum())\nSTAR_centers= np.array(list(STAR_centers))\nSTAR_centers\n\n\n\nQSO = cleaned[cleaned[\"class\"]==2].drop(\"class\",axis=1)\nQSO_centers = map(lambda a: a/QSO.shape[0],QSO.sum())\nQSO_centers= np.array(list(QSO_centers))\nQSO_centers\n\n\n\nCode\nm1=model.fit([galaxy_centers,STAR_centers,QSO_centers])\nm1.labels_\n\n\nCode\npred= m1.predict(X_test)\nprint(classification_report(y_test, pred))"
  },
  {
    "objectID": "models/KNN_and_XGB.html",
    "href": "models/KNN_and_XGB.html",
    "title": "KNN & XGB",
    "section": "",
    "text": "Code\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\nplt.rcParams[\"figure.dpi\"] = 170\nplt.style.use(\"seaborn\")\n\nimport xgboost as xgb\nfrom sklearn.model_selection import (\n    train_test_split,\n    KFold,\n    GridSearchCV,\n    cross_val_score,\n    RandomizedSearchCV,\n    RepeatedStratifiedKFold,\n)\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.inspection import permutation_importance\n\n# import warnings filter\nfrom warnings import simplefilter\n\n# ignore all future warnings\nsimplefilter(action=\"ignore\", category=FutureWarning)"
  },
  {
    "objectID": "models/KNN_and_XGB.html#basic-knn-with-5-fold-cv",
    "href": "models/KNN_and_XGB.html#basic-knn-with-5-fold-cv",
    "title": "KNN & XGB",
    "section": "Basic KNN with 5-Fold CV",
    "text": "Basic KNN with 5-Fold CV\n\n\nCode\nscaler = StandardScaler()\n# standardize all columns\nX_train_std = scaler.fit_transform(X_train)\n\n# create KNN model\nknn = KNeighborsClassifier(n_neighbors=5)\n\n# create 5-Fold CV\nkfold = RepeatedStratifiedKFold(n_splits=5,random_state=123)\n\n# Fit the model\nresults = cross_val_score(knn, X_train_std, y_train, cv=kfold, scoring='accuracy')\n\nprint(max(results))\n\n\n0.9341875"
  },
  {
    "objectID": "models/KNN_and_XGB.html#tuning-the-knn-model",
    "href": "models/KNN_and_XGB.html#tuning-the-knn-model",
    "title": "KNN & XGB",
    "section": "Tuning the KNN Model",
    "text": "Tuning the KNN Model\n\nCode\n# hyper parameters for CV\nhyper_params = {\n    'n_neighbors': range(1, 10+1)\n}\n\nscaler = StandardScaler()\n# standardize all columns\nX_train_std = scaler.fit_transform(X_train)\n\n# create KNN model\nknn = KNeighborsClassifier()\n\n# create 5-Fold CV\nkfold = RepeatedStratifiedKFold(n_splits=5,random_state=123)\n\n# Tune `knn` using grid search\ngrid_search = GridSearchCV(knn, hyper_params, cv=kfold, scoring='accuracy')\ngrid_results = grid_search.fit(X_train_std, y_train)\n\n\n\nCode\n# get the best accuracy achieved\nprint(\"Best accuracy\", grid_results.best_score_)\nprint(\"Best K value\", grid_results.best_estimator_.get_params()['n_neighbors'])\n\n\nBest accuracy 0.9297842857142856\nBest K value 3\n\n\n\n\nCode\nplt.plot(hyper_params['n_neighbors'], grid_search.cv_results_['mean_test_score'])\nplt.title('Cross validated grid search results'.title())\nplt.show()\n\n\n\n\n\n\nFeature Interpretation\n\nCode\n%%capture\n\n# fit the KNN with best K value\nknn_best = KNeighborsClassifier(n_neighbors=3)\nknn_best_fit = knn.fit(X_train_std, y_train)\n\nr = permutation_importance(\n    knn,\n    X_train_std,\n    y_train,\n    n_repeats=5,\n    random_state=123,\n    n_jobs=-1,\n)\n\n\n\nCode\nfeat = pd.DataFrame({\n    'feature': X_train.columns,\n    'importance': r.importances_mean,\n}).sort_values('importance')\n\nplt.scatter(data=feat, x='importance', y='feature')\nplt.xlabel(\"Importance\")\nplt.title('feature interpretation'.title())\nplt.show()"
  },
  {
    "objectID": "models/KNN_and_XGB.html#basic-gradient-boosting-model",
    "href": "models/KNN_and_XGB.html#basic-gradient-boosting-model",
    "title": "KNN & XGB",
    "section": "Basic Gradient Boosting Model",
    "text": "Basic Gradient Boosting Model\n\n\nCode\n# create XGBClassifier model\nxgb_model = xgb.XGBClassifier()\n\n# create 5-Fold CV\nkfold = KFold(n_splits=5, random_state=123, shuffle=True)\n\n# Fit the model\nresults = cross_val_score(xgb_model, X_train, y_train, cv=kfold, scoring='accuracy')\n\nprint(max(results))\n\n\n0.9791428571428571\n\n\n\nWith Randomized Searching\n\nCode\nparam_distributions = {\n    'n_estimators': [5000, 5500, 6000],\n    'learning_rate': [0.001, 0.01, 0.1],\n    'max_depth': [9, 10, 11, 12],\n    'min_child_weight': [1, 2, 3]\n}\n\nrandom_search = RandomizedSearchCV(\n    xgb_model,\n    param_distributions,\n    n_iter=15,\n    cv=kfold,\n    scoring='accuracy',\n    random_state=123,\n    n_jobs=-1,\n)\n\nsearch_results = random_search.fit(X_train_std, y_train)\n\n\n\nCode\nsearch_results.best_score_, search_results.best_params_\n\n\n(0.9795,\n {'n_estimators': 6000,\n  'min_child_weight': 1,\n  'max_depth': 9,\n  'learning_rate': 0.01})\n\n\n\n\nBest Model & Feature Interpretation\n\nCode\nbest_model = xgb.XGBRFClassifier(\n    n_estimators=6000,\n    learning_rate=0.01,\n    max_depth=9,\n    min_child_weight=1,\n    subsample=1,\n    colsample_bytree=0.75,\n    colsample_bylevel=0.75,\n    colsample_bynode=0.75\n)\n\nbest_model_fit = best_model.fit(X_train, y_train)\n\n\n\nCode\nfeat = pd.DataFrame({\n    'feature': X_train.columns,\n    'importance': best_model_fit.feature_importances_,\n}).sort_values('importance')\n\n\nplt.scatter(data=feat, x='importance', y='feature')\nplt.xlabel(\"Importance\")\nplt.title('feature interpretation'.title())\nplt.show()"
  },
  {
    "objectID": "models/svm-model.html",
    "href": "models/svm-model.html",
    "title": "SVM",
    "section": "",
    "text": "Trying out the support vector machine algorithm on the stellar-classification dataset, to successfully predict the target variable.\n\n\n\nCode\n# import the required libraries\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport matplotlib.patches as mpatches\n\nfrom sklearn.svm import SVC\nfrom sklearn.metrics import accuracy_score\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.compose import ColumnTransformer\nfrom sklearn.compose import make_column_selector as selector\nfrom sklearn.preprocessing import PowerTransformer\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.preprocessing import OneHotEncoder\n\nimport tensorflow as tf\n\n\nCode\n# load in the dataset\n# note: uncomment the following lines to download the dataset\n# !kaggle datasets download -d fedesoriano/stellar-classification-dataset-sdss17\n# !unzip stellar-classification-dataset-sdss17.zip\n# !rm stellar-classification-dataset-sdss17.zip\n# !echo 'Dataset Downloaded Successfully'\n\n\n\nCode\n# read in the dataset\nstellar = pd.read_csv(\"star_classification.csv\")\nstellar.head()\n\n\n\n\n\n\n  \n    \n      \n      obj_ID\n      alpha\n      delta\n      u\n      g\n      r\n      i\n      z\n      run_ID\n      rerun_ID\n      cam_col\n      field_ID\n      spec_obj_ID\n      class\n      redshift\n      plate\n      MJD\n      fiber_ID\n    \n  \n  \n    \n      0\n      1.237661e+18\n      135.689107\n      32.494632\n      23.87882\n      22.27530\n      20.39501\n      19.16573\n      18.79371\n      3606\n      301\n      2\n      79\n      6.543777e+18\n      GALAXY\n      0.634794\n      5812\n      56354\n      171\n    \n    \n      1\n      1.237665e+18\n      144.826101\n      31.274185\n      24.77759\n      22.83188\n      22.58444\n      21.16812\n      21.61427\n      4518\n      301\n      5\n      119\n      1.176014e+19\n      GALAXY\n      0.779136\n      10445\n      58158\n      427\n    \n    \n      2\n      1.237661e+18\n      142.188790\n      35.582444\n      25.26307\n      22.66389\n      20.60976\n      19.34857\n      18.94827\n      3606\n      301\n      2\n      120\n      5.152200e+18\n      GALAXY\n      0.644195\n      4576\n      55592\n      299\n    \n    \n      3\n      1.237663e+18\n      338.741038\n      -0.402828\n      22.13682\n      23.77656\n      21.61162\n      20.50454\n      19.25010\n      4192\n      301\n      3\n      214\n      1.030107e+19\n      GALAXY\n      0.932346\n      9149\n      58039\n      775\n    \n    \n      4\n      1.237680e+18\n      345.282593\n      21.183866\n      19.43718\n      17.58028\n      16.49747\n      15.97711\n      15.54461\n      8102\n      301\n      3\n      137\n      6.891865e+18\n      GALAXY\n      0.116123\n      6121\n      56187\n      842\n    \n  \n\n\n\n\n\n\nCode\n# statistical overview of the dataset\nstellar.info()\n\n\n<class 'pandas.core.frame.DataFrame'>\nRangeIndex: 100000 entries, 0 to 99999\nData columns (total 18 columns):\n #   Column       Non-Null Count   Dtype  \n---  ------       --------------   -----  \n 0   obj_ID       100000 non-null  float64\n 1   alpha        100000 non-null  float64\n 2   delta        100000 non-null  float64\n 3   u            100000 non-null  float64\n 4   g            100000 non-null  float64\n 5   r            100000 non-null  float64\n 6   i            100000 non-null  float64\n 7   z            100000 non-null  float64\n 8   run_ID       100000 non-null  int64  \n 9   rerun_ID     100000 non-null  int64  \n 10  cam_col      100000 non-null  int64  \n 11  field_ID     100000 non-null  int64  \n 12  spec_obj_ID  100000 non-null  float64\n 13  class        100000 non-null  object \n 14  redshift     100000 non-null  float64\n 15  plate        100000 non-null  int64  \n 16  MJD          100000 non-null  int64  \n 17  fiber_ID     100000 non-null  int64  \ndtypes: float64(10), int64(7), object(1)\nmemory usage: 13.7+ MB\n\n\nGreat! Looks like we have no observations with missing values\n\n\nCode\n# closer look at the features\n\nresult = {}\nfeatures = stellar.columns\n\nfor feature in features:\n    if stellar[feature].nunique() > 10:\n        result[feature] = stellar[feature].nunique()\n    else:\n        result[feature] = stellar[feature].unique()\nresult\n\n\n{'obj_ID': 78053,\n 'alpha': 99999,\n 'delta': 99999,\n 'u': 93748,\n 'g': 92651,\n 'r': 91901,\n 'i': 92019,\n 'z': 92007,\n 'run_ID': 430,\n 'rerun_ID': array([301]),\n 'cam_col': array([2, 5, 3, 4, 6, 1]),\n 'field_ID': 856,\n 'spec_obj_ID': 100000,\n 'class': array(['GALAXY', 'QSO', 'STAR'], dtype=object),\n 'redshift': 99295,\n 'plate': 6284,\n 'MJD': 2180,\n 'fiber_ID': 1000}\n\n\nWe see that there are 3 values for our traget variable class, namely:\n\nGALAXY\nQSO\nSTART\n\n\n\nCode\n# number of observations\nlen(stellar)\n\n\n100000\n\n\n\nCode\n# reorder the observations based on the class variable\nstellar.sort_values('class', axis=0, ascending=True, inplace=True)\n\n\n\nCode\n# distribution of the classe's observations\n# view the layout of the dataset\n\nresult = {}\ntarget_labels = stellar['class'].tolist()\n\nfor target in target_labels:\n    result[target] = -1\n\nfor index, obs in enumerate(stellar.values):\n    if result[obs[13]] == -1:\n        result[obs[13]] = index\nresult\n\n\n{'GALAXY': 0, 'QSO': 59445, 'STAR': 78406}\n\n\nWe can now proceed to viewing the distribution of each class in a plot\n\n\nCode\n# visualize using the features: `redshift` and `alpha`\nclass_labels = stellar['class']\nscaled_features = StandardScaler().fit_transform(stellar.drop('class', axis=1))\nscaled_stellar = pd.DataFrame(scaled_features, index=stellar.index, columns=(stellar.drop('class', axis=1)).columns)\n\nscaled_stellar['class'] = class_labels\nscaled_stellar\n\n\n\n\n\n\n  \n    \n      \n      obj_ID\n      alpha\n      delta\n      u\n      g\n      r\n      i\n      z\n      run_ID\n      rerun_ID\n      cam_col\n      field_ID\n      spec_obj_ID\n      redshift\n      plate\n      MJD\n      fiber_ID\n      class\n    \n  \n  \n    \n      0\n      -0.445634\n      -0.434604\n      0.425529\n      0.059755\n      0.054926\n      0.403962\n      0.046007\n      0.003937\n      -0.445535\n      0.0\n      -0.952553\n      -0.718947\n      0.228609\n      0.079557\n      0.228633\n      0.423203\n      -1.021342\n      GALAXY\n    \n    \n      58230\n      -1.139298\n      -0.368710\n      1.379317\n      -0.000092\n      0.031505\n      0.903290\n      1.010382\n      0.053337\n      -1.139260\n      0.0\n      -0.322395\n      -0.121673\n      0.731651\n      0.485616\n      0.731633\n      0.802528\n      0.986019\n      GALAXY\n    \n    \n      58228\n      -1.139298\n      -0.381213\n      1.351881\n      0.045294\n      0.047447\n      0.809590\n      0.974606\n      0.053707\n      -1.139260\n      0.0\n      -0.322395\n      -0.161939\n      0.731256\n      -0.439343\n      0.731294\n      0.805846\n      -1.516760\n      GALAXY\n    \n    \n      58227\n      -1.070015\n      0.480361\n      -1.322346\n      0.006342\n      0.031496\n      0.476575\n      0.233830\n      0.007300\n      -1.070040\n      0.0\n      0.307763\n      -0.584728\n      -0.378349\n      -0.092486\n      -0.378354\n      -0.141914\n      0.108945\n      GALAXY\n    \n    \n      58226\n      -1.070015\n      0.405437\n      -1.327688\n      0.112584\n      0.011681\n      -0.286498\n      -0.398618\n      -0.022590\n      -1.070040\n      0.0\n      0.307763\n      -0.906853\n      -1.429054\n      -0.218501\n      -1.429064\n      -1.760957\n      0.090596\n      GALAXY\n    \n    \n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n    \n    \n      46329\n      -1.092601\n      0.766323\n      0.734251\n      0.045131\n      0.016127\n      0.035459\n      0.100766\n      0.011344\n      -1.092435\n      0.0\n      -1.582710\n      -0.953830\n      1.893737\n      -0.789715\n      1.893782\n      1.470494\n      -1.509421\n      STAR\n    \n    \n      46330\n      -0.296055\n      0.617464\n      0.094237\n      -0.139661\n      -0.129397\n      -2.011636\n      -1.917283\n      -0.096359\n      -0.295898\n      0.0\n      -1.582710\n      3.294198\n      -0.907107\n      -0.789845\n      -0.907096\n      -0.577640\n      -0.691063\n      STAR\n    \n    \n      46331\n      -2.124159\n      -0.701998\n      -1.226114\n      0.014384\n      -0.030837\n      -0.636966\n      -0.606623\n      -0.028993\n      -2.124116\n      0.0\n      -0.322395\n      -1.088049\n      0.783445\n      -0.788944\n      0.783457\n      0.626690\n      -0.335096\n      STAR\n    \n    \n      75082\n      -1.700567\n      -0.203089\n      -1.300428\n      -0.000616\n      0.010098\n      0.515621\n      0.775514\n      0.049766\n      -1.700653\n      0.0\n      0.937920\n      -1.020939\n      -0.457924\n      -0.787913\n      -0.457953\n      -0.174538\n      1.173177\n      STAR\n    \n    \n      68798\n      1.760822\n      -1.593640\n      0.337726\n      0.050271\n      0.103026\n      1.872407\n      1.077138\n      0.040500\n      1.760848\n      0.0\n      -0.322395\n      -0.960541\n      0.878618\n      -0.789890\n      0.878637\n      1.405246\n      -0.654366\n      STAR\n    \n  \n\n100000 rows × 18 columns\n\n\n\n\n\nCode\n\nx_data = scaled_stellar['i']\ny_data = scaled_stellar['redshift']\n\ngalaxy_x = x_data[:59445]\ngalaxy_y = y_data[:59445]\n\nqso_x = x_data[59445:78406]\nqso_y = y_data[59445:78406]\n\nstar_x = x_data[78406:]\nstar_y = y_data[78406:]\n\n# create the figure\nplt.figure(figsize=(10,7))\nplt.scatter(galaxy_x,galaxy_y,marker='+',color='green')\nplt.scatter(qso_x,qso_y,marker='_',color='red')\nplt.scatter(star_x,star_y,marker='*',color='blue')\n\nplt.title(\"Target Distribution\")\nplt.xlabel(\"Green filter\")\nplt.ylabel(\"Redshift\")\nplt.show()\n\n\n\n\n\n\n\n\nCode\n# encode values for class column\nstellar.replace({'class': {'GALAXY': 0, 'STAR': 1, 'QSO':2}}, inplace=True)\n\n# remove all columns containing ID at the end\ncleaned = stellar.drop(stellar.filter(regex='ID$').columns, axis=1)\n\n# drop the date column\ncleaned = stellar.drop('MJD', axis=1)\n\n\nCode\n# make the X and y varialbes (all features)\nX_all = cleaned.drop('class', axis=1)\ny = cleaned['class']\n\n\nCode\n# keep the 2 features\nfeatures = cleaned.columns.tolist()\nfeatures.remove('redshift')\nfeatures.remove('g')\n\n# make the X and y varialbes\nX_specifc = cleaned.drop(features, axis=1)\n\n\nCode\nsc = StandardScaler()\nyj = PowerTransformer(method=\"yeo-johnson\")\n\npreprocessor = ColumnTransformer([(\"norm\", yj, selector(dtype_include=\"number\")),\n                (\"std_encode\", sc, selector(dtype_include=\"number\"))\n                ])\n\n\n\n\n\n\nUsing all features\n\n\nCode\n# initialize an SVC model (all features)\nX_train, X_test, y_train, y_test = train_test_split(X_all, y, train_size=0.7, random_state=123)\n\nsvc_clf = SVC(kernel='rbf')\nsvc_model_pipeline = Pipeline(steps=[\n  (\"preprocessor\", preprocessor),\n  (\"model\", svc_clf),\n])\n# train the model on our dataset\nsvc_model_pipeline.fit(X_train,y_train)\n# store the predictions of the model\ny_pred = svc_model_pipeline.predict(X_test)\n# evaluate the model on accuracy\nprint(accuracy_score(y_test,y_pred))\n\n\n\nCode\n# initialize an SVC model (2 features)\nX_train, X_test, y_train, y_test = train_test_split(X_specifc, y, train_size=0.7, random_state=123)\n\nsvc_clf = SVC(kernel='rbf')\n# train the model on our dataset\nsvc_model_pipeline = Pipeline(steps=[\n  (\"preprocessor\", preprocessor),\n  (\"model\", svc_clf),\n])\nsvc_model_pipeline.fit(X_train,y_train)\n# store the predictions of the model\ny_pred = svc_model_pipeline.predict(X_test)\n# evaluate the model on accuracy\nprint(accuracy_score(y_test,y_pred))\n\n\n0.9644666666666667\n\n\n\n\nCode\nfrom sklearn.metrics import ConfusionMatrixDisplay\nConfusionMatrixDisplay.from_predictions(y_test, y_pred)\n\n\n<sklearn.metrics._plot.confusion_matrix.ConfusionMatrixDisplay at 0x14560f730>\n\n\n\n\n\n\n\nCode\n# {'class': {'GALAXY': 0, 'STAR': 1, 'QSO':2}}\n\n\narray([0, 2, 1])\n\n\n\n\n\n\nCode\n# re-read in the dataset\nstellar = pd.read_csv(\"star_classification.csv\")\nstellar.head()\n\n\n\n\n\n\n  \n    \n      \n      obj_ID\n      alpha\n      delta\n      u\n      g\n      r\n      i\n      z\n      run_ID\n      rerun_ID\n      cam_col\n      field_ID\n      spec_obj_ID\n      class\n      redshift\n      plate\n      MJD\n      fiber_ID\n    \n  \n  \n    \n      0\n      1.237661e+18\n      135.689107\n      32.494632\n      23.87882\n      22.27530\n      20.39501\n      19.16573\n      18.79371\n      3606\n      301\n      2\n      79\n      6.543777e+18\n      GALAXY\n      0.634794\n      5812\n      56354\n      171\n    \n    \n      1\n      1.237665e+18\n      144.826101\n      31.274185\n      24.77759\n      22.83188\n      22.58444\n      21.16812\n      21.61427\n      4518\n      301\n      5\n      119\n      1.176014e+19\n      GALAXY\n      0.779136\n      10445\n      58158\n      427\n    \n    \n      2\n      1.237661e+18\n      142.188790\n      35.582444\n      25.26307\n      22.66389\n      20.60976\n      19.34857\n      18.94827\n      3606\n      301\n      2\n      120\n      5.152200e+18\n      GALAXY\n      0.644195\n      4576\n      55592\n      299\n    \n    \n      3\n      1.237663e+18\n      338.741038\n      -0.402828\n      22.13682\n      23.77656\n      21.61162\n      20.50454\n      19.25010\n      4192\n      301\n      3\n      214\n      1.030107e+19\n      GALAXY\n      0.932346\n      9149\n      58039\n      775\n    \n    \n      4\n      1.237680e+18\n      345.282593\n      21.183866\n      19.43718\n      17.58028\n      16.49747\n      15.97711\n      15.54461\n      8102\n      301\n      3\n      137\n      6.891865e+18\n      GALAXY\n      0.116123\n      6121\n      56187\n      842\n    \n  \n\n\n\n\n\nFeature Engineering\n\n\nCode\n# encode values for class column\nstellar.replace({'class': {'GALAXY': 0, 'STAR': 1, 'QSO':2}}, inplace=True)\n\n# remove all columns containing ID at the end\ncleaned = stellar.drop(stellar.filter(regex='ID$').columns, axis=1)\n\n# drop the date column\ncleaned = cleaned.drop('MJD', axis=1)\n\n# make the X and y varialbes\nX = cleaned.drop(['class'], axis=1)\ny = cleaned['class']\n\n# split the dataset\nX_train, X_test, y_train, y_test = train_test_split(X, y, train_size=0.7, random_state=123)\n\n\n\nCode\nX_train\n\n\n\n\n\n\n  \n    \n      \n      alpha\n      delta\n      u\n      g\n      r\n      i\n      z\n      cam_col\n      redshift\n      plate\n    \n  \n  \n    \n      52308\n      48.573323\n      -0.609684\n      18.80211\n      16.75706\n      15.66640\n      15.20351\n      14.80488\n      2\n      0.115262\n      412\n    \n    \n      27380\n      260.094957\n      27.014951\n      20.26815\n      18.68293\n      18.09193\n      17.86272\n      17.77524\n      1\n      -0.000209\n      2193\n    \n    \n      94588\n      135.477289\n      37.187920\n      24.05482\n      21.67747\n      20.38921\n      19.56683\n      19.27142\n      2\n      0.538213\n      4608\n    \n    \n      7361\n      118.180524\n      9.267706\n      20.56594\n      19.53276\n      19.22361\n      18.98544\n      18.89350\n      4\n      0.000171\n      2945\n    \n    \n      52298\n      15.024146\n      4.027261\n      24.63314\n      20.48507\n      18.76055\n      18.10046\n      18.07149\n      1\n      0.322435\n      4309\n    \n    \n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n    \n    \n      63206\n      115.884742\n      20.139136\n      18.93230\n      17.66927\n      17.21911\n      16.99153\n      16.88261\n      4\n      0.000022\n      1263\n    \n    \n      61404\n      133.430333\n      -0.719735\n      19.83167\n      19.90659\n      19.58608\n      19.57696\n      19.64975\n      2\n      1.309977\n      12533\n    \n    \n      17730\n      255.223294\n      23.638965\n      18.99455\n      18.04598\n      17.63787\n      17.44783\n      17.38555\n      3\n      -0.000624\n      3290\n    \n    \n      28030\n      232.458207\n      36.143802\n      20.20453\n      18.46623\n      17.64739\n      17.25447\n      16.98376\n      3\n      0.065986\n      1401\n    \n    \n      15725\n      5.646502\n      3.562572\n      23.04084\n      22.26374\n      22.04945\n      21.24024\n      20.42934\n      6\n      1.211771\n      9444\n    \n  \n\n70000 rows × 10 columns\n\n\n\n\nCode\nsc = StandardScaler()\nyj = PowerTransformer(method=\"yeo-johnson\")\n\npreprocessor = ColumnTransformer([(\"normalization\", yj, selector(dtype_include=\"number\")),\n                (\"standardization\", sc, selector(dtype_include=\"number\")),\n                ])\n\n\nBuilding a model\n\n\nCode\ndeep_model = tf.keras.Sequential([\n    tf.keras.layers.Dense(512, activation='tanh'),\n    tf.keras.layers.Dense(64, activation='tanh'),\n    tf.keras.layers.Dense(3, activation='softmax')\n])\n\n\nCode\n# compile the model\ndeep_model.compile(\n    loss= 'sparse_categorical_crossentropy',\n    optimizer= tf.keras.optimizers.SGD(),\n    metrics= ['accuracy']\n)\n\n\nCode\n# create the pipeline\ndeep_model_pipeline = Pipeline(steps=[\n  (\"preprocessor\", preprocessor),\n  (\"model\", deep_model),\n])\n\n\n\nCode\ndeep_model_pipeline\n\n\nPipeline(steps=[('preprocessor',\n                 ColumnTransformer(transformers=[('normalization',\n                                                  PowerTransformer(),\n                                                  <sklearn.compose._column_transformer.make_column_selector object at 0x148b62980>),\n                                                 ('standardization',\n                                                  StandardScaler(),\n                                                  <sklearn.compose._column_transformer.make_column_selector object at 0x148b61b40>)])),\n                ('model',\n                 <keras.engine.sequential.Sequential object at 0x148b61ab0>)])In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.PipelinePipeline(steps=[('preprocessor',\n                 ColumnTransformer(transformers=[('normalization',\n                                                  PowerTransformer(),\n                                                  <sklearn.compose._column_transformer.make_column_selector object at 0x148b62980>),\n                                                 ('standardization',\n                                                  StandardScaler(),\n                                                  <sklearn.compose._column_transformer.make_column_selector object at 0x148b61b40>)])),\n                ('model',\n                 <keras.engine.sequential.Sequential object at 0x148b61ab0>)])preprocessor: ColumnTransformerColumnTransformer(transformers=[('normalization', PowerTransformer(),\n                                 <sklearn.compose._column_transformer.make_column_selector object at 0x148b62980>),\n                                ('standardization', StandardScaler(),\n                                 <sklearn.compose._column_transformer.make_column_selector object at 0x148b61b40>)])normalization<sklearn.compose._column_transformer.make_column_selector object at 0x148b62980>PowerTransformerPowerTransformer()standardization<sklearn.compose._column_transformer.make_column_selector object at 0x148b61b40>StandardScalerStandardScaler()Sequential<keras.engine.sequential.Sequential object at 0x148b61ab0>\n\n\n\n\nCode\ndeep_model.summary()\n\n\nModel: \"sequential_37\"\n_________________________________________________________________\n Layer (type)                Output Shape              Param #   \n=================================================================\n dense_114 (Dense)           (None, 512)               10752     \n                                                                 \n dense_115 (Dense)           (None, 64)                32832     \n                                                                 \n dense_116 (Dense)           (None, 3)                 195       \n                                                                 \n=================================================================\nTotal params: 43,779\nTrainable params: 43,779\nNon-trainable params: 0\n_________________________________________________________________\n\n\n\n\nCode\n# fit the model\ndeep_model_history = deep_model_pipeline.fit(X_train,y_train, model__epochs=10, model__validation_split=0.2)\n\n\nEpoch 1/10\n1750/1750 [==============================] - 13s 7ms/step - loss: 0.2560 - accuracy: 0.9258 - val_loss: 0.1757 - val_accuracy: 0.9546\nEpoch 2/10\n1750/1750 [==============================] - 15s 9ms/step - loss: 0.1625 - accuracy: 0.9552 - val_loss: 0.1501 - val_accuracy: 0.9616\nEpoch 3/10\n1750/1750 [==============================] - 15s 8ms/step - loss: 0.1418 - accuracy: 0.9596 - val_loss: 0.1336 - val_accuracy: 0.9631\nEpoch 4/10\n1750/1750 [==============================] - 16s 9ms/step - loss: 0.1318 - accuracy: 0.9618 - val_loss: 0.1279 - val_accuracy: 0.9635\nEpoch 5/10\n1750/1750 [==============================] - 20s 11ms/step - loss: 0.1261 - accuracy: 0.9629 - val_loss: 0.1235 - val_accuracy: 0.9656\nEpoch 6/10\n1750/1750 [==============================] - 21s 12ms/step - loss: 0.1225 - accuracy: 0.9641 - val_loss: 0.1209 - val_accuracy: 0.9651\nEpoch 7/10\n1750/1750 [==============================] - 21s 12ms/step - loss: 0.1199 - accuracy: 0.9644 - val_loss: 0.1174 - val_accuracy: 0.9665\nEpoch 8/10\n1750/1750 [==============================] - 23s 13ms/step - loss: 0.1177 - accuracy: 0.9649 - val_loss: 0.1145 - val_accuracy: 0.9672\nEpoch 9/10\n1750/1750 [==============================] - 25s 14ms/step - loss: 0.1161 - accuracy: 0.9654 - val_loss: 0.1140 - val_accuracy: 0.9674\nEpoch 10/10\n1750/1750 [==============================] - 26s 15ms/step - loss: 0.1142 - accuracy: 0.9662 - val_loss: 0.1124 - val_accuracy: 0.9677\n\n\n\n\nCode\ny_pred = deep_model_pipeline.predict(X_test)\naccuracy_score(y_test, y_pred.argmax(axis=-1))\n\n\n0.9713333333333334"
  },
  {
    "objectID": "models/Random_forest.html",
    "href": "models/Random_forest.html",
    "title": "Random Forest",
    "section": "",
    "text": "Code\n# utility packages\nimport matplotlib.pyplot as plt\nimport numpy as np\nimport pandas as pd\n\n\n# modeling packages\nfrom sklearn.model_selection import train_test_split, KFold, RepeatedKFold, cross_val_score\nfrom sklearn.model_selection import GridSearchCV\nfrom sklearn.metrics import mean_squared_error, roc_auc_score\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn import metrics\n\n\nCode\n# read the dataset\ndf = pd.read_csv('./star_classification 2.csv')\n\n# encode values for class column\ndf.replace({'class': {'GALAXY': 0, 'STAR': 1, 'QSO':2}}, inplace=True)\n\n# remove all columns containing ID at the end\ncleaned = df.drop(df.filter(regex='ID$').columns, axis=1)\n# drop the date column\ncleaned = cleaned.drop('MJD', axis=1)\n\ncleaned = cleaned.reset_index()\n\n# make the X and y varialbes\nX = cleaned.drop('class', axis=1)\ny = cleaned['class']\n\n# split the dataset\nX_train, X_test, y_train, y_test = train_test_split(X, y, train_size=0.7, random_state=123)\n\n\n\n\n\nCode\n# Trial 1\n# Trying number of tress 10, 20, 30, ...\n\n\n\nCode\nn = []\naccuracy = []\nfor i in range(10, 1001, 10):\n    clf=RandomForestClassifier(n_estimators=i)\n    clf.fit(X_train,y_train)\n    y_pred=clf.predict(X_test)\n    print(f\"#{i} Accuracy:\",metrics.accuracy_score(y_test, y_pred))\n    n.append(i)\n    accuracy.append(metrics.accuracy_score(y_test, y_pred))\n\n\n#10 Accuracy: 0.9771333333333333\n#20 Accuracy: 0.9789\n#30 Accuracy: 0.9789333333333333\n#40 Accuracy: 0.9793666666666667\n#50 Accuracy: 0.9789\n#60 Accuracy: 0.9791666666666666\n#70 Accuracy: 0.9791666666666666\n#80 Accuracy: 0.9791\n#90 Accuracy: 0.9792\n#100 Accuracy: 0.9793333333333333\n#110 Accuracy: 0.9791\n#120 Accuracy: 0.9791333333333333\n#130 Accuracy: 0.9789333333333333\n#140 Accuracy: 0.9793666666666667\n#150 Accuracy: 0.9788333333333333\n#160 Accuracy: 0.9796666666666667\n#170 Accuracy: 0.9790333333333333\n#180 Accuracy: 0.9790333333333333\n#190 Accuracy: 0.9791\n#200 Accuracy: 0.9794666666666667\n#210 Accuracy: 0.9794\n#220 Accuracy: 0.9793333333333333\n#230 Accuracy: 0.9792\n#240 Accuracy: 0.9795\n#250 Accuracy: 0.9792666666666666\n#260 Accuracy: 0.9793\n#270 Accuracy: 0.9792333333333333\n#280 Accuracy: 0.9794\n#290 Accuracy: 0.9795333333333334\n#300 Accuracy: 0.9792\n#310 Accuracy: 0.9792666666666666\n#320 Accuracy: 0.9794666666666667\n#330 Accuracy: 0.9791\n#340 Accuracy: 0.9792333333333333\n#350 Accuracy: 0.9793\n#360 Accuracy: 0.9795333333333334\n#370 Accuracy: 0.9794333333333334\n#380 Accuracy: 0.9792666666666666\n#390 Accuracy: 0.9791333333333333\n#400 Accuracy: 0.9791666666666666\n#410 Accuracy: 0.9789666666666667\n#420 Accuracy: 0.9793666666666667\n#430 Accuracy: 0.9795666666666667\n#440 Accuracy: 0.9794333333333334\n#450 Accuracy: 0.9797\n#460 Accuracy: 0.9792\n#470 Accuracy: 0.9796\n#480 Accuracy: 0.9794333333333334\n#490 Accuracy: 0.9790333333333333\n#500 Accuracy: 0.9793666666666667\n#510 Accuracy: 0.9792\n#520 Accuracy: 0.9794\n#530 Accuracy: 0.9792666666666666\n#540 Accuracy: 0.9792666666666666\n#550 Accuracy: 0.9794\n#560 Accuracy: 0.9795333333333334\n#570 Accuracy: 0.9794\n#580 Accuracy: 0.9792666666666666\n#590 Accuracy: 0.9793333333333333\n#600 Accuracy: 0.9790333333333333\n#610 Accuracy: 0.9795\n#620 Accuracy: 0.9792\n#630 Accuracy: 0.9793333333333333\n#640 Accuracy: 0.9790666666666666\n#650 Accuracy: 0.9793\n#660 Accuracy: 0.9792666666666666\n#670 Accuracy: 0.9792333333333333\n#680 Accuracy: 0.9790666666666666\n#690 Accuracy: 0.9792333333333333\n#700 Accuracy: 0.9793\n#710 Accuracy: 0.9797\n#720 Accuracy: 0.9793666666666667\n#730 Accuracy: 0.9793\n#740 Accuracy: 0.9792666666666666\n#750 Accuracy: 0.9794333333333334\n#760 Accuracy: 0.9795\n#770 Accuracy: 0.9792333333333333\n#780 Accuracy: 0.9793666666666667\n#790 Accuracy: 0.9793333333333333\n#800 Accuracy: 0.9791666666666666\n#810 Accuracy: 0.9794\n#820 Accuracy: 0.9794333333333334\n#830 Accuracy: 0.9794333333333334\n#840 Accuracy: 0.9793333333333333\n#850 Accuracy: 0.9794666666666667\n#860 Accuracy: 0.9792333333333333\n#870 Accuracy: 0.9793666666666667\n#880 Accuracy: 0.9794\n#890 Accuracy: 0.9793\n#900 Accuracy: 0.9794333333333334\n#910 Accuracy: 0.9795666666666667\n#920 Accuracy: 0.9795666666666667\n#930 Accuracy: 0.9792333333333333\n#940 Accuracy: 0.9793333333333333\n#950 Accuracy: 0.9792666666666666\n#960 Accuracy: 0.9792333333333333\n#970 Accuracy: 0.9793666666666667\n#980 Accuracy: 0.9794666666666667\n#990 Accuracy: 0.9795\n#1000 Accuracy: 0.9793\n\n\n\n\nCode\nmax(accuracy)\n# accuracy = 0.9797\n\n\n0.9797\n\n\n\nCode\nn.iloc(max(accuracy))\n# n = 260 \n\n\nCode\n# Trial 2\n# Using grid search to find the optimal paramerts\n\n\nCode\n# Random forest with Grid Search for paramerts tuning\nrfc = RandomForestClassifier()\nparameters = {\n    \"n_estimators\":[5,10,50,100,250],\n    \"max_depth\":[2,4,8,16,32,None]\n}\n\n\n\nCode\ncv = GridSearchCV(rfc, parameters, cv=5)\ncv.fit(X_train, y_train)\n\n\nGridSearchCV(cv=5, estimator=RandomForestClassifier(),\n             param_grid={'max_depth': [2, 4, 8, 16, 32, None],\n                         'n_estimators': [5, 10, 50, 100, 250]})\n\n\n\n\nCode\n # The highest accuracy achieved\nprint(cv.best_score_)\n\n\n0.977997852463188\n\n\n\n\nCode\n # The paramerts that yeild the best score\nprint(cv.best_params_)\n\n\n{'max_depth': None, 'n_estimators': 250}\n\n\nQuestions: - What is the best n? - how does random forest prevent overfitting? via bootstrap samples and bagging - what is rf bad at? can be black box, not so good at regression - what is rf good at?\nObservations: * Quick to compute (average computing time for different RF with different no of trees and depths is less than a min) * Diffrent value after each run though its close * Highest accuraccy (0.9799) when 250-260 tree and no max depth (20m) * 3h 1-1000 tress w max 0.9797 * RF doesnt assume any distribution\n\nCode\n# Trial 3\n# Trying different set of columns\n\n\nCode\n# read the dataset\ndf = pd.read_csv('./star_classification 2.csv')\n\n# encode values for class column\ndf.replace({'class': {'GALAXY': 0, 'STAR': 1, 'QSO':2}}, inplace=True)\n\n# remove unneeded columns\ncleaned = df.drop(['spec_obj_ID','run_ID', 'field_ID', 'plate', 'MJD'], axis=1)\n\ncleaned = cleaned.reset_index()\n\n# make the X and y varialbes\nX = cleaned.drop('class', axis=1)\ny = cleaned['class']\n\n# split the dataset\nX_train, X_test, y_train, y_test = train_test_split(X, y, train_size=0.7, random_state=123)\n\n\nCode\n# Random forest with Grid Search for paramerts tuning\nrfc = RandomForestClassifier()\nparameters = {\n    \"n_estimators\":[5,10,50,100,250],\n    \"max_depth\":[2,4,8,16,32,None]\n}\n\n\n\nCode\ncv = GridSearchCV(rfc, parameters, cv=5)\ncv.fit(X_train, y_train)\n\n\nGridSearchCV(cv=5, estimator=RandomForestClassifier(),\n             param_grid={'max_depth': [2, 4, 8, 16, 32, None],\n                         'n_estimators': [5, 10, 50, 100, 250]})\n\n\n\n\nCode\n # The highest accuracy achieved\nprint(cv.best_score_)\n\n\n0.9767428571428571\n\n\n\n\nCode\n # The paramerts that yeild the best score\nprint(cv.best_params_)\n\n\n{'max_depth': None, 'n_estimators': 100}"
  },
  {
    "objectID": "models/featureEngineering.html",
    "href": "models/featureEngineering.html",
    "title": "Feature Engineering",
    "section": "",
    "text": "Code\n# read the dataset\ndf = pd.read_csv(\"..\\data\\star_classification.csv\")\n\n# encode values for class column\ndf.replace({'class': {'GALAXY': 0, 'STAR': 1, 'QSO':2}}, inplace=True)\n\n\n\n\nCode\nf,ax = plt.subplots(figsize=(12,8))\nsns.heatmap(df.corr(), cmap=\"PuBu\", annot=True, linewidths=0.5, fmt= '.2f',ax=ax)\nplt.show()\n\n\n\n\n\n\n\nCode\ndf.corr()[\"class\"].sort_values()\n\n\nfield_ID      -0.038044\nu             -0.017701\ng             -0.005915\nrun_ID        -0.000049\nobj_ID        -0.000047\nalpha          0.004552\ncam_col        0.014476\nz              0.017352\nfiber_ID       0.032053\ndelta          0.056643\nr              0.150691\nMJD            0.207262\nspec_obj_ID    0.215722\nplate          0.215722\ni              0.284396\nredshift       0.536822\nclass          1.000000\nrerun_ID            NaN\nName: class, dtype: float64\n\n\n\n\nCode\nfrom dis import dis\n\ncleaned = df.drop(['obj_ID','run_ID','rerun_ID',\"alpha\"], axis = 1)\n\ncleaned=cleaned.drop(79543)\n# make the X and y varialbes\nX = cleaned.drop('class', axis=1)\ny = cleaned['class']\ndisplay(cleaned)\n\n\nsm = SMOTE(random_state=42)\nX, y = sm.fit_resample(X, y)\n\nX_train, X_test, y_train, y_test = train_test_split(X, y, train_size=0.7, random_state=123)\n\n\n\n\n\n\n  \n    \n      \n      delta\n      u\n      g\n      r\n      i\n      z\n      cam_col\n      field_ID\n      spec_obj_ID\n      class\n      redshift\n      plate\n      MJD\n      fiber_ID\n    \n  \n  \n    \n      0\n      32.494632\n      23.87882\n      22.27530\n      20.39501\n      19.16573\n      18.79371\n      2\n      79\n      6.543777e+18\n      0\n      0.634794\n      5812\n      56354\n      171\n    \n    \n      1\n      31.274185\n      24.77759\n      22.83188\n      22.58444\n      21.16812\n      21.61427\n      5\n      119\n      1.176014e+19\n      0\n      0.779136\n      10445\n      58158\n      427\n    \n    \n      2\n      35.582444\n      25.26307\n      22.66389\n      20.60976\n      19.34857\n      18.94827\n      2\n      120\n      5.152200e+18\n      0\n      0.644195\n      4576\n      55592\n      299\n    \n    \n      3\n      -0.402828\n      22.13682\n      23.77656\n      21.61162\n      20.50454\n      19.25010\n      3\n      214\n      1.030107e+19\n      0\n      0.932346\n      9149\n      58039\n      775\n    \n    \n      4\n      21.183866\n      19.43718\n      17.58028\n      16.49747\n      15.97711\n      15.54461\n      3\n      137\n      6.891865e+18\n      0\n      0.116123\n      6121\n      56187\n      842\n    \n    \n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n    \n    \n      99995\n      -2.594074\n      22.16759\n      22.97586\n      21.90404\n      21.30548\n      20.73569\n      2\n      581\n      1.055431e+19\n      0\n      0.000000\n      9374\n      57749\n      438\n    \n    \n      99996\n      19.798874\n      22.69118\n      22.38628\n      20.45003\n      19.75759\n      19.41526\n      1\n      289\n      8.586351e+18\n      0\n      0.404895\n      7626\n      56934\n      866\n    \n    \n      99997\n      15.700707\n      21.16916\n      19.26997\n      18.20428\n      17.69034\n      17.35221\n      4\n      308\n      3.112008e+18\n      0\n      0.143366\n      2764\n      54535\n      74\n    \n    \n      99998\n      46.660365\n      25.35039\n      21.63757\n      19.91386\n      19.07254\n      18.62482\n      4\n      131\n      7.601080e+18\n      0\n      0.455040\n      6751\n      56368\n      470\n    \n    \n      99999\n      49.464643\n      22.62171\n      21.79745\n      20.60115\n      20.00959\n      19.28075\n      4\n      60\n      8.343152e+18\n      0\n      0.542944\n      7410\n      57104\n      851\n    \n  \n\n99999 rows × 14 columns\n\n\n\n\npre-processing\n\nCode\n# Normalizing approach\nyj = PowerTransformer(method=\"yeo-johnson\")\nscaler = StandardScaler()\n# nzv_encoder = VarianceThreshold(threshold=0.1)\n# pca = PCA(n_components=7)\n\n# Normalize all numeric features\npreprocessor = ColumnTransformer([(\"norm\", yj, selector(dtype_include=\"number\")),\n                (\"std_encode\", scaler, selector(dtype_include=\"number\")),\n                # (\"nzv_encoder\", nzv_encoder, selector(dtype_include=\"number\")),\n                # (\"pca_encode\", pca, selector(dtype_include=\"number\"))\n                ])\n\n\n\nRandom Forest Classifier\n\n\nCode\n\n#best training data for Random Forest Classifier\ntree_X_train= X_train[['delta', 'u', 'g', 'i',\"spec_obj_ID\", 'redshift', 'plate']]\ntree_X_test= X_test[['delta', 'u', 'g', 'i',\"spec_obj_ID\", 'redshift', 'plate']]\n\n\n#creating the Random Forest Classifier model with the pre processing steps\nr_forest = RandomForestClassifier()\nr_forest_pipeline = Pipeline(steps=[\n  (\"norm\", yj),\n  (\"std_encode\",scaler),\n  (\"r_forest\", r_forest),\n])\n\n\n# training the model\nr_forest_pipeline.fit(tree_X_train,y_train)\npredicted = r_forest_pipeline.predict(tree_X_test)\nscore = r_forest_pipeline.score(tree_X_test,y_test)\nr_forest_pipeline_score = np.mean(score)\nr_forest_pipeline_score\n\n\n\nc:\\Users\\user\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\numpy\\core\\_methods.py:236: RuntimeWarning: overflow encountered in multiply\n\n\n0.9825049999065438\n\n\n\n\nCode\nprint(classification_report(y_test,r_forest_pipeline.predict(tree_X_test)))\n\n\n              precision    recall  f1-score   support\n\n           0       0.97      0.98      0.97     17862\n           1       1.00      1.00      1.00     17804\n           2       0.98      0.97      0.98     17835\n\n    accuracy                           0.98     53501\n   macro avg       0.98      0.98      0.98     53501\nweighted avg       0.98      0.98      0.98     53501\n\n\n\n\n\nCode\nr_forest.predict(tree_X_test)\n\n\nc:\\Users\\user\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\base.py:443: UserWarning: X has feature names, but RandomForestClassifier was fitted without feature names\n\n\narray([2, 2, 2, ..., 2, 2, 2], dtype=int64)\n\n\n\nCode\n# define loss function\nscoring = 'accuracy'\n\n# create 10 fold CV object\nkfold = KFold(n_splits=5, random_state=123, shuffle=True)\n\n# # fit model with 10-fold CV\nresults = cross_val_score(r_forest_pipeline, X_test, y_test, cv=kfold, scoring=scoring)\nresults.mean()\n\n\n\nCode\n#feature selection\ntsfs=SFS(r_forest_pipeline,k_features=14,scoring=scoring,cv=kfold)\ntsfs.fit(X,y)\ntsfs.subsets_\n\n\n#best output\n#  7: {'feature_idx': (2, 3, 4, 6, 12, 13, 14),\n#   'cv_scores': array([0.98320576, 0.98357025, 0.9836824 , 0.98457958, 0.98340202]),\n#   'avg_score': 0.9836880029158606,\n#   'feature_names': ('delta','u','g','i','spec_obj_ID','redshift','plate')},\n\n\nTypeError: 'module' object is not callable\n\n\n\nCode\n# Create grid of hyperparameter values\n\n# Number of trees in random forest\nn_estimators = [int(x) for x in np.linspace(start = 200, stop = 2000, num = 10)]\n# Number of features to consider at every split\n# Maximum number of levels in tree\nmax_depth = [int(x) for x in np.linspace(10, 110, num = 11)]\nmax_depth.append(None)\n# Minimum number of samples required to split a node\nmin_samples_split = [2, 5, 10]\n# Minimum number of samples required at each leaf node\nmin_samples_leaf = [1, 2, 4]\n# Method of selecting samples for training each tree\nbootstrap = [True, False]\n\nhyper_grid = {'r_forest__n_estimators': n_estimators,\n               'r_forest__max_depth': max_depth,\n               'r_forest__min_samples_split': min_samples_split,\n               'r_forest__min_samples_leaf': min_samples_leaf,\n               'r_forest__bootstrap': bootstrap}\n\n\ngrid_search = GridSearchCV(r_forest_pipeline, hyper_grid, cv=kfold, scoring=scoring)\nresults = grid_search.fit(X_train[[\"g\",\"i\",\"redshift\"]], y_train)\n\n\n\nSVM\n\n\nCode\n\n#best training data for Random Forest Classifier\n#??\n\n\n#creating SVM model\nsvm_clf = svm.SVC(kernel='rbf', C=2, random_state=0)\nmodel_pipeline = Pipeline(steps=[\n  (\"norm\", yj),\n  (\"std_encode\",scaler),\n  (\"knn\", svm_clf),\n])\n\n\n#training SVM model\n# model_pipeline.fit(X_train,y_train)\n# predicted = model_pipeline.predict(X_test)\n# score = model_pipeline.score(X_test,y_test)\n# model_pipeline_score = np.mean(score)\n# model_pipeline_score\n#0.977\n\n\n0.973589278705071\n\n\n\n\nCode\n# define loss function\nscoring = 'accuracy'\n\n# create 10 fold CV object\nkfold = KFold(n_splits=5, random_state=123, shuffle=True)\n\n# # fit model with 10-fold CV\n# results = cross_val_score(model_pipeline, X_test, y_test, cv=kfold, scoring=scoring)\n# results.mean()\n\n#feature selection\nsvmsfs=SFS(model_pipeline,\n    k_features=17,\n    scoring=scoring,\n    cv=kfold)\nsvmsfs.fit(X,y)\nsvmsfs.subsets_\n\n\nNameError: name 'SFS' is not defined\n\n\n\n\nLogistic Regression\n\n\nCode\n#best training data for Logistic Regression\nlog_X_train=X_train[[\"delta\",\"u\",\"g\",\"r\",\"cam_col\",\"field_ID\",\"spec_obj_ID\",\"redshift\",\"plate\",\"MJD\",\"fiber_ID\"]]\nlog_X_test=X_test[[\"delta\",\"u\",\"g\",\"r\",\"cam_col\",\"field_ID\",\"spec_obj_ID\",\"redshift\",\"plate\",\"MJD\",\"fiber_ID\"]]\n\n#creating Logistic Regression model\nlog_reg=LogisticRegression(max_iter=1000,C=4714.85,penalty=\"l2\")\nlog_reg_pipeline = Pipeline(steps=[\n  (\"norm\", yj),\n  (\"std_encode\",scaler),\n  (\"log_reg\", log_reg),\n])\n\n#training Logistic Regression model\nlog_reg_pipeline.fit(log_X_train,y_train)\npredicted = log_reg_pipeline.predict(log_X_test)\nscore = log_reg_pipeline.score(log_X_test,y_test)\nlog_reg_pipeline_score = np.mean(score)\nlog_reg_pipeline_score\n\n\nc:\\Users\\user\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\numpy\\core\\_methods.py:236: RuntimeWarning: overflow encountered in multiply\n\n\n0.9682062017532382\n\n\n\n\nCode\n# define scoring function\nscoring = 'accuracy'\n\n# create 10 fold CV object\nkfold = KFold(n_splits=5, random_state=123, shuffle=True)\n\n#  fit model with 10-fold CV\nresults = cross_val_score(log_reg_pipeline, X_test, y_test, cv=kfold, scoring=scoring)\nresults.mean()\n\n\n\nc:\\Users\\user\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\numpy\\core\\_methods.py:236: RuntimeWarning: overflow encountered in multiply\nc:\\Users\\user\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\numpy\\core\\_methods.py:236: RuntimeWarning: overflow encountered in multiply\nc:\\Users\\user\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\numpy\\core\\_methods.py:236: RuntimeWarning: overflow encountered in multiply\nc:\\Users\\user\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\numpy\\core\\_methods.py:236: RuntimeWarning: overflow encountered in multiply\nc:\\Users\\user\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\numpy\\core\\_methods.py:236: RuntimeWarning: overflow encountered in multiply\n\n\n0.9693089963642143\n\n\n\nCode\n# Create grid of hyperparameter values\nC = np.logspace(-4, 4, 50)\npenalty = ['l1', 'l2']\n\nhyper_grid = {'log_reg__C': C,\n        'log_reg__penalty':penalty,\n        }\n\n# Tune a Logistic Regression model using grid search\ngrid_search = GridSearchCV(model_pipeline, hyper_grid, cv=kfold, scoring=scoring)\nresults = grid_search.fit(X_train, y_train)\n\nresults.best_params_\n#best output\n#C=4714.85\n#penalty=\"l2\"\n\n\n\nCode\nfrom mlxtend.feature_selection import sequential_feature_selector as SFS\n\n#feature selection\nsfs=SFS(log_reg_pipeline,\n    k_features=14,\n    scoring=scoring,\n    cv=kfold,\n    forward=True\n    )\n    \nsfs.fit(X,y)\nsfs.subsets_\n\n\n#best output\n#  13: {'feature_idx': ( 1, 2, 3, 4, 5, 7, 8, 9, 10, 11, 12, 13),\n#   'cv_scores': array([0.96859842, 0.96991617, 0.96851431, 0.97128999, 0.9696358 ]),\n#   'avg_score': 0.9695909384024448,\n#   'feature_names': ('delta','u','g','r','i','cam_col','field_ID','spec_obj_ID','redshift','plate','MJD','fiber_ID')}}\n\n\nTypeError: 'module' object is not callable\n\n\n\n\nCode\nprint(classification_report(y_test,log_reg_pipeline.predict(log_X_test)))\n\n\n              precision    recall  f1-score   support\n\n           0       0.94      0.96      0.95     17862\n           1       0.99      1.00      1.00     17804\n           2       0.97      0.94      0.96     17835\n\n    accuracy                           0.97     53501\n   macro avg       0.97      0.97      0.97     53501\nweighted avg       0.97      0.97      0.97     53501\n\n\n\n\n\nK-Nearest Neighbors\n\n\nCode\n#best training data for K-Nearest Neighbors\nknn_X_train=X_train[['g', 'r', 'i', 'z', 'redshift', 'plate']]\nknn_X_test=X_test[['g', 'r', 'i', 'z', 'redshift', 'plate']]\n\n\n#creating the Knn model\nknn=KNeighborsClassifier(n_neighbors=3)\nknn_pipeline = Pipeline(steps=[\n  (\"norm\", yj),\n  (\"std_encode\",scaler),\n  (\"knn\", knn),\n])\n\n#training the KNN model\nknn_pipeline.fit(knn_X_train,y_train)\npredicted = knn_pipeline.predict(knn_X_test)\nscore = knn_pipeline.score(knn_X_test,y_test)\nknn_pipeline_score = np.mean(score)\nknn_pipeline_score\n\n\n0.9730285415225883\n\n\n\nCode\n# define loss function\nscoring = 'accuracy'\n\n# create 10 fold CV object\nkfold = KFold(n_splits=5, random_state=123, shuffle=True)\n\n# # fit model with 10-fold CV\nresults = cross_val_score(model_pipeline, X_test, y_test, cv=kfold, scoring=scoring)\nresults.mean()\n\n\n\nCode\n# hyper parameters for CV\nhyper_params = {\n    'n_neighbors': range(1, 10+1)\n}\n\n# Tune `knn` using grid search\ngrid_search = GridSearchCV(knn_pipeline, hyper_params, cv=kfold, scoring='accuracy')\ngrid_results = grid_search.fit(log_X_train, y_train)\n\n#best output\n# 3\n\n\nCode\n#feature selection\nsfs=SFS(knn_pipeline,\n    k_features=13,\n    scoring=scoring,\n    cv=kfold)\n    \nsfs.fit(X,y)\nsfs.subsets_\n\n#best output\n#  {'feature_idx': (4, 5, 6, 7, 13, 14),\n#   'cv_scores': array([0.97300025, 0.97330866, 0.97364511, 0.975075  , 0.97473855]),\n#   'avg_score': 0.9739535144531359,\n#   'feature_names': ('g', 'r', 'i', 'z', 'redshift', 'plate')}, \n\n\n\nCode\nprint(classification_report(y_test,knn_pipeline.predict(knn_X_test)))\n\n\n              precision    recall  f1-score   support\n\n           0       0.96      0.96      0.96     17862\n           1       0.98      1.00      0.99     17804\n           2       0.98      0.97      0.97     17835\n\n    accuracy                           0.97     53501\n   macro avg       0.97      0.97      0.97     53501\nweighted avg       0.97      0.97      0.97     53501\n\n\n\n\n\ndeep learining\n\nCode\nfrom dis import dis\n\n# cleaned = df.drop(['obj_ID','alpha','delta','run_ID','rerun_ID','cam_col','field_ID','fiber_ID'], axis = 1)\ncleaned = df.drop(['u','r','i','z','obj_ID','spec_obj_ID','MJD'], axis = 1)\n\n# cleaned = df.drop(df.filter(regex='ID$').columns, axis=1)\n# drop the date column\n# cleaned = cleaned.drop([\"MJD\",\"plate\",\"cam_col\"], axis=1)\ncleaned=cleaned.drop(79543)\n# make the X and y varialbes\nX = cleaned.drop('class', axis=1)\ny = cleaned['class']\ndisplay(cleaned)\n\n\nsm = SMOTE(random_state=42)\nX, y = sm.fit_resample(X, y)\n\n\nX_train, X_test, y_train, y_test = train_test_split(X, y, train_size=0.7, random_state=123)\n\nfrom tensorflow.keras import utils\n# y = utils.to_categorical(y)\n# y_train = utils.to_categorical(y_train)\n# y_test = utils.to_categorical(y_test)\n\n\nCode\n\n# define the keras model\nmodel = Sequential()\nmodel.add(Dense(units=64, input_dim=20, activation=\"tanh\"))\nmodel.add(Dense(units=64,  activation=\"tanh\"))\nmodel.add(Dense(units=32,  activation=\"tanh\"))\nmodel.add(Dense(units=3, activation='softmax'))\n\n# compile the keras model\nmodel.compile(\n    loss='sparse_categorical_crossentropy', \n    optimizer=\"rmsprop\",\n    metrics='accuracy'\n)\n# fit the model\n# model.fit(X, y, epochs=20, validation_split=0.2)\nmodel_pipeline = Pipeline(steps=[\n  (\"preprocessor\", preprocessor),\n  (\"model\", model),\n])\nm1=model_pipeline.fit(X_train,y_train, model__epochs=20, model__validation_split=0.2,model__batch_size=32,)\n\n\nCode\npredicted=m1.predict(X_test)\ny_classes = predicted.argmax(axis=-1)\n\n# model_pipeline.transform(X_test)\nprint(classification_report(y_test, y_classes)) \n\n\nCode\n# define loss function\nloss = 'accuracy'\n\n# create 10 fold CV object\nkfold = KFold(n_splits=5, random_state=123, shuffle=True)\n\n# fit model with 10-fold CV\nresults = cross_val_score(model_pipeline, X_train, y_train, cv=kfold, scoring=loss)\nresults\n\n\nCode\n# hyper_grid = {'n_neighbors': range(2, 26)}\n# grid_search = GridSearchCV(knn, hyper_grid, cv=kfold, scoring=loss)\n# results = grid_search.fit(X_train, y_train)\n\n\n\nk-means clustring | trash\n\nCode\nmodel = sklearn.cluster.KMeans(n_clusters=3,random_state=123)\nmodel\n\n\nCode\ngalaxy = cleaned[cleaned[\"class\"]==0].drop(\"class\",axis=1)\ngalaxy_centers = map(lambda a: a/galaxy.shape[0],galaxy.sum())\ngalaxy_centers= np.array(list(galaxy_centers))\ngalaxy_centers\n\n\nSTAR = cleaned[cleaned[\"class\"]==1].drop(\"class\",axis=1)\nSTAR_centers = map(lambda a: a/STAR.shape[0],STAR.sum())\nSTAR_centers= np.array(list(STAR_centers))\nSTAR_centers\n\n\n\nQSO = cleaned[cleaned[\"class\"]==2].drop(\"class\",axis=1)\nQSO_centers = map(lambda a: a/QSO.shape[0],QSO.sum())\nQSO_centers= np.array(list(QSO_centers))\nQSO_centers\n\n\n\nCode\nm1=model.fit([galaxy_centers,STAR_centers,QSO_centers])\nm1.labels_\n\n\nCode\npred= m1.predict(X_test)\nprint(classification_report(y_test, pred))"
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "the Stellar Classification Dataset - SDSS17data was obtained from kaggle\nthe dataset has the following attributes\n\n\n\n\n\n\n\n\n\ncolumn\ndescription\n\n\n\n\n1\nobj_ID\nObject Identifier, the unique value that identifies the object in the image catalog used by the CAS\n\n\n2\nalpha\nRight Ascension angle (at J2000 epoch)\n\n\n3\ndelta\nDeclination angle (at J2000 epoch)\n\n\n4\nu\nUltraviolet filter in the photometric system\n\n\n5\ng\nGreen filter in the photometric system\n\n\n6\nr\nRed filter in the photometric system\n\n\n7\ni\nNear Infrared filter in the photometric system\n\n\n8\nz\nInfrared filter in the photometric system\n\n\n9\nrun_ID\nRun Number used to identify the specific scan\n\n\n10\nrereun_ID\nRerun Number to specify how the image was processed\n\n\n11\ncam_col\nCamera column to identify the scanline within the run\n\n\n12\nfield_ID\nField number to identify each field\n\n\n13\nspec_obj_ID\nUnique ID used for optical spectroscopic objects (this means that 2 different observations with the same spec_obj_ID must share the output class)\n\n\n14\nclass\nobject class (galaxy, star or quasar object)\n\n\n15\nredshift\nredshift value based on the increase in wavelength\n\n\n16\nplate\nplate ID, identifies each plate in SDSS"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Stellar Classification",
    "section": "",
    "text": "The Stellar Classification Dataset - SDSS17 consists of 100,000 observations with 17 features that allows for a stellar object to be classified as a star, galaxy or quasar.\nTarget variable : the stellar classification ( star, galaxy or quasar)"
  },
  {
    "objectID": "index.html#team-members",
    "href": "index.html#team-members",
    "title": "Stellar Classification",
    "section": "Team members",
    "text": "Team members\nAhmed Almohammed Turki Alsaedi Sultan Alkadhi Salman Al-Harbi Ammar Alfaifi Lana almorabah"
  }
]