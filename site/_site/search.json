[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Stellar Classification",
    "section": "",
    "text": "The Stellar Classification Dataset - SDSS17 consists of 100,000 observations with 17 features that allows for a stellar object to be classified as a star, galaxy or quasar.\nTarget variable : the stellar classification ( star, galaxy or quasar)"
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "the Stellar Classification Dataset - SDSS17data was obtained from kaggle\nthe dataset has the following attributes\n\n\n\n\n\n\n\n\n\ncolumn\ndescription\n\n\n\n\n1\nobj_ID\nObject Identifier, the unique value that identifies the object in the image catalog used by the CAS\n\n\n2\nalpha\nRight Ascension angle (at J2000 epoch)\n\n\n3\ndelta\nDeclination angle (at J2000 epoch)\n\n\n4\nu\nUltraviolet filter in the photometric system\n\n\n5\ng\nGreen filter in the photometric system\n\n\n6\nr\nRed filter in the photometric system\n\n\n7\ni\nNear Infrared filter in the photometric system\n\n\n8\nz\nInfrared filter in the photometric system\n\n\n9\nrun_ID\nRun Number used to identify the specific scan\n\n\n10\nrereun_ID\nRerun Number to specify how the image was processed\n\n\n11\ncam_col\nCamera column to identify the scanline within the run\n\n\n12\nfield_ID\nField number to identify each field\n\n\n13\nspec_obj_ID\nUnique ID used for optical spectroscopic objects (this means that 2 different observations with the same spec_obj_ID must share the output class)\n\n\n14\nclass\nobject class (galaxy, star or quasar object)\n\n\n15\nredshift\nredshift value based on the increase in wavelength\n\n\n16\nplate\nplate ID, identifies each plate in SDSS"
  },
  {
    "objectID": "index.html#data",
    "href": "index.html#data",
    "title": "Stellar Classification",
    "section": "Data",
    "text": "Data\nthe Stellar Classification Dataset - SDSS17data was obtained from kaggle\nthe dataset has the following attributes\n\n\n\n\n\n\n\n\n\ncolumn\ndescription\n\n\n\n\n1\nobj_ID\nObject Identifier, the unique value that identifies the object in the image catalog used by the CAS\n\n\n2\nalpha\nRight Ascension angle (at J2000 epoch)\n\n\n3\ndelta\nDeclination angle (at J2000 epoch)\n\n\n4\nu\nUltraviolet filter in the photometric system\n\n\n5\ng\nGreen filter in the photometric system\n\n\n6\nr\nRed filter in the photometric system\n\n\n7\ni\nNear Infrared filter in the photometric system\n\n\n8\nz\nInfrared filter in the photometric system\n\n\n9\nrun_ID\nRun Number used to identify the specific scan\n\n\n10\nrereun_ID\nRerun Number to specify how the image was processed\n\n\n11\ncam_col\nCamera column to identify the scanline within the run\n\n\n12\nfield_ID\nField number to identify each field\n\n\n13\nspec_obj_ID\nUnique ID used for optical spectroscopic objects (this means that 2 different observations with the same spec_obj_ID must share the output class)\n\n\n14\nclass\nobject class (galaxy, star or quasar object)\n\n\n15\nredshift\nredshift value based on the increase in wavelength\n\n\n16\nplate\nplate ID, identifies each plate in SDSS"
  },
  {
    "objectID": "index.html#team-members",
    "href": "index.html#team-members",
    "title": "Stellar Classification",
    "section": "Team members",
    "text": "Team members\nAhmed Almohammed Turki Alsaedi Sultan Alkadhi Salman Al-Harbi Ammar Alfaifi Lana almorabah"
  },
  {
    "objectID": "KNN_and_XGB.html",
    "href": "KNN_and_XGB.html",
    "title": "KNN & XGB",
    "section": "",
    "text": "Code\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\nplt.rcParams[\"figure.dpi\"] = 170\nplt.style.use(\"seaborn\")\n\nimport xgboost as xgb\nfrom sklearn.model_selection import (\n    train_test_split,\n    KFold,\n    GridSearchCV,\n    cross_val_score,\n    RandomizedSearchCV,\n    RepeatedStratifiedKFold,\n)\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.inspection import permutation_importance\n\n# import warnings filter\nfrom warnings import simplefilter\n\n# ignore all future warnings\nsimplefilter(action=\"ignore\", category=FutureWarning)"
  },
  {
    "objectID": "KNN_and_XGB.html#basic-knn-with-5-fold-cv",
    "href": "KNN_and_XGB.html#basic-knn-with-5-fold-cv",
    "title": "KNN & XGB",
    "section": "Basic KNN with 5-Fold CV",
    "text": "Basic KNN with 5-Fold CV\n\n\nCode\nscaler = StandardScaler()\n# standardize all columns\nX_train_std = scaler.fit_transform(X_train)\n\n# create KNN model\nknn = KNeighborsClassifier(n_neighbors=5)\n\n# create 5-Fold CV\nkfold = RepeatedStratifiedKFold(n_splits=5,random_state=123)\n\n# Fit the model\nresults = cross_val_score(knn, X_train_std, y_train, cv=kfold, scoring='accuracy')\n\nprint(max(results))\n\n\n0.9341875"
  },
  {
    "objectID": "KNN_and_XGB.html#tuning-the-knn-model",
    "href": "KNN_and_XGB.html#tuning-the-knn-model",
    "title": "KNN & XGB",
    "section": "Tuning the KNN Model",
    "text": "Tuning the KNN Model\n\nCode\n# hyper parameters for CV\nhyper_params = {\n    'n_neighbors': range(1, 10+1)\n}\n\nscaler = StandardScaler()\n# standardize all columns\nX_train_std = scaler.fit_transform(X_train)\n\n# create KNN model\nknn = KNeighborsClassifier()\n\n# create 5-Fold CV\nkfold = RepeatedStratifiedKFold(n_splits=5,random_state=123)\n\n# Tune `knn` using grid search\ngrid_search = GridSearchCV(knn, hyper_params, cv=kfold, scoring='accuracy')\ngrid_results = grid_search.fit(X_train_std, y_train)\n\n\n\nCode\n# get the best accuracy achieved\nprint(\"Best accuracy\", grid_results.best_score_)\nprint(\"Best K value\", grid_results.best_estimator_.get_params()['n_neighbors'])\n\n\nBest accuracy 0.9297842857142856\nBest K value 3\n\n\n\n\nCode\nplt.plot(hyper_params['n_neighbors'], grid_search.cv_results_['mean_test_score'])\nplt.title('Cross validated grid search results'.title())\nplt.show()\n\n\n\n\n\n\nFeature Interpretation\n\nCode\n%%capture\n\n# fit the KNN with best K value\nknn_best = KNeighborsClassifier(n_neighbors=3)\nknn_best_fit = knn.fit(X_train_std, y_train)\n\nr = permutation_importance(\n    knn,\n    X_train_std,\n    y_train,\n    n_repeats=5,\n    random_state=123,\n    n_jobs=-1,\n)\n\n\n\nCode\nfeat = pd.DataFrame({\n    'feature': X_train.columns,\n    'importance': r.importances_mean,\n}).sort_values('importance')\n\nplt.scatter(data=feat, x='importance', y='feature')\nplt.xlabel(\"Importance\")\nplt.title('feature interpretation'.title())\nplt.show()"
  },
  {
    "objectID": "KNN_and_XGB.html#basic-gradient-boosting-model",
    "href": "KNN_and_XGB.html#basic-gradient-boosting-model",
    "title": "KNN & XGB",
    "section": "Basic Gradient Boosting Model",
    "text": "Basic Gradient Boosting Model\n\n\nCode\n# create XGBClassifier model\nxgb_model = xgb.XGBClassifier()\n\n# create 5-Fold CV\nkfold = KFold(n_splits=5, random_state=123, shuffle=True)\n\n# Fit the model\nresults = cross_val_score(xgb_model, X_train, y_train, cv=kfold, scoring='accuracy')\n\nprint(max(results))\n\n\n0.9791428571428571\n\n\n\nWith Randomized Searching\n\nCode\nparam_distributions = {\n    'n_estimators': [5000, 5500, 6000],\n    'learning_rate': [0.001, 0.01, 0.1],\n    'max_depth': [9, 10, 11, 12],\n    'min_child_weight': [1, 2, 3]\n}\n\nrandom_search = RandomizedSearchCV(\n    xgb_model,\n    param_distributions,\n    n_iter=15,\n    cv=kfold,\n    scoring='accuracy',\n    random_state=123,\n    n_jobs=-1,\n)\n\nsearch_results = random_search.fit(X_train_std, y_train)\n\n\n\nCode\nsearch_results.best_score_, search_results.best_params_\n\n\n(0.9795,\n {'n_estimators': 6000,\n  'min_child_weight': 1,\n  'max_depth': 9,\n  'learning_rate': 0.01})\n\n\n\n\nBest Model & Feature Interpretation\n\nCode\nbest_model = xgb.XGBRFClassifier(\n    n_estimators=6000,\n    learning_rate=0.01,\n    max_depth=9,\n    min_child_weight=1,\n    subsample=1,\n    colsample_bytree=0.75,\n    colsample_bylevel=0.75,\n    colsample_bynode=0.75\n)\n\nbest_model_fit = best_model.fit(X_train, y_train)\n\n\n\nCode\nfeat = pd.DataFrame({\n    'feature': X_train.columns,\n    'importance': best_model_fit.feature_importances_,\n}).sort_values('importance')\n\n\nplt.scatter(data=feat, x='importance', y='feature')\nplt.xlabel(\"Importance\")\nplt.title('feature interpretation'.title())\nplt.show()"
  },
  {
    "objectID": "LogisticRegression.html",
    "href": "LogisticRegression.html",
    "title": "LogisticRegression",
    "section": "",
    "text": "Code\n#####lana \n\nplt.figure(figsize=(13,7))\nsns.heatmap(df.corr(),annot=True,vmin=-1,vmax=1)\n\n\nCode\n# read the dataset\ndf = pd.read_csv(\"archive\\star_classification.csv\")\n\n# encode values for class column\ndf.replace({'class': {'GALAXY': 0, 'STAR': 1, 'QSO':2}}, inplace=True)\n\n# # remove all columns containing ID at the end\n# cleaned = df.drop(df.filter(regex='ID$').columns, axis=1)\n# # drop the date column\n# cleaned = cleaned.drop(['MJD',\"plate\"], axis=1)\n# cleaned=cleaned.drop(79543)\n# # make the X and y varialbes\n# X = cleaned.drop('class', axis=1)\n# y = cleaned['class']\n\n# # split the dataset\n# X_train, X_test, y_train, y_test = train_test_split(X, y, train_size=0.7, random_state=123)\n\n\nCode\ndf\n\n\nCode\ndf.corr().style.background_gradient(cmap=\"coolwarm\")\n\n\nCode\ndf.corr()[\"class\"].sort_values()\n\n\nCode\nfrom dis import dis\ncleaned = df.drop(['u ','r','i','z','obj_ID','spec_obj_ID','MJD'], axis = 1)\n\n# cleaned = df.drop(['obj_ID','alpha','delta','run_ID','rerun_ID','cam_col','field_ID','fiber_ID'], axis = 1)\n# cleaned = df.drop(df.filter(regex='ID$').columns, axis=1)\n# drop the date column\n# cleaned = cleaned.drop([\"u\",\"r\",\"z\",\"alpha\",\"delta\",\"MJD\"], axis=1)\ncleaned=cleaned.drop(79543)\n# make the X and y varialbes\nX = cleaned.drop('class', axis=1)\ny = cleaned['class']\ndisplay(cleaned)\n\n\nsm = SMOTE(random_state=42)\nX, y = sm.fit_resample(X, y)\n\nX_train, X_test, y_train, y_test = train_test_split(X, y, train_size=0.7, random_state=123)\n\n\nCode\nsns.displot(df[\"run_ID\"]);\n\n\nCode\nsns.displot(df[\"field_ID\"]);\n\n\nCode\nsns.displot(df[\"MJD\"]);\n\n\nCode\ncleaned[cleaned[\"redshift\"]>5]\n\n\nCode\nsns.displot(cleaned[\"alpha\"]);\n\n\nCode\nsns.displot(cleaned[\"delta\"]);\n\n\nCode\nsns.displot(x=cleaned[\"u\"]);\n\n\nCode\ncleaned\n\n\nCode\nsns.displot(x=cleaned[\"g\"]);\n\n\nCode\nsns.displot(cleaned[\"r\"]);\n\n\nCode\nsns.displot(cleaned[\"i\"]);\n\n\nCode\nsns.displot(cleaned[\"z\"]);\n\n\nCode\nsns.countplot(x=cleaned[\"cam_col\"]);\n\n\nCode\nsns.countplot(x=cleaned[\"class\"]);\n\n\nCode\nsns.displot(cleaned[\"redshift\"]);\n\n\nCode\nsns.displot(cleaned[\"plate\"]);\n\n\npre-processing\n\nCode\n# Normalizing approach\nyj = PowerTransformer(method=\"yeo-johnson\")\nscaler = StandardScaler()\n# nzv_encoder = VarianceThreshold(threshold=0.1)\n# pca = PCA(n_components=7)\n# Normalize all numeric features\npreprocessor = ColumnTransformer([(\"norm\", yj, selector(dtype_include=\"number\")),\n                (\"std_encode\", scaler, selector(dtype_include=\"number\")),\n                # (\"nzv_encoder\", nzv_encoder, selector(dtype_include=\"number\")),\n                # (\"pca_encode\", pca, selector(dtype_include=\"number\"))\n                ])\n\n\n\nRandom Forest Classifier\n\nCode\nr_forest = RandomForestClassifier()\nmodel_pipeline = Pipeline(steps=[\n  (\"preprocessor\", preprocessor),\n  (\"knn\", r_forest),\n])\nmodel_pipeline.fit(X_train,y_train)\npredicted = model_pipeline.predict(X_test)\nscore = model_pipeline.score(X_test,y_test)\nmodel_pipeline_score = np.mean(score)\nmodel_pipeline_score\n0.983\n\n\nCode\nprint(classification_report(y_test, predicted))\n\n\n\nSVM\n\nCode\nsvm_clf = svm.SVC(kernel='rbf', C=32, random_state=0)\nmodel_pipeline = Pipeline(steps=[\n  (\"preprocessor\", preprocessor),\n  (\"knn\", svm_clf),\n])\nmodel_pipeline.fit(X_train,y_train)\npredicted = model_pipeline.predict(X_test)\nscore = model_pipeline.score(X_test,y_test)\nmodel_pipeline_score = np.mean(score)\nmodel_pipeline_score\n#0.977\n\n\nCode\nprint(classification_report(y_test, predicted)) \n\n\n\nLogistic Regression\n\nCode\nlog_reg=LogisticRegression()\nmodel_pipeline = Pipeline(steps=[\n  (\"preprocessor\", preprocessor),\n  (\"log_reg\", log_reg),\n])\nmodel_pipeline.fit(X_train,y_train)\npredicted = model_pipeline.predict(X_test)\nscore = model_pipeline.score(X_test,y_test)\nmodel_pipeline_score = np.mean(score)\nmodel_pipeline_score\n\n\nCode\nprint(classification_report(y_test, predicted)) \n\n\nCode\n# define loss function\nloss = 'accuracy'\n\n# create 10 fold CV object\nkfold = KFold(n_splits=5, random_state=123, shuffle=True)\n\n# fit model with 10-fold CV\n# results = cross_val_score(model_pipeline, X_train, y_train, cv=kfold, scoring=loss)\n# results\n\n\nCode\nresults.mean()\n\n\nCode\n# Create grid of hyperparameter values\nhyper_grid = {'log_reg__max_iter': range(100, 1000,100)}\n\n# Tune a knn model using grid search\ngrid_search = GridSearchCV(model_pipeline, hyper_grid, cv=kfold, scoring=loss)\nresults = grid_search.fit(X_train, y_train)\n\n\nCode\nresults.best_params_\n\n\n\nK-Nearest Neighbors\n\nCode\n\nknn=KNeighborsClassifier(n_neighbors=3)\nmodel_pipeline = Pipeline(steps=[\n  (\"preprocessor\", preprocessor),\n  (\"knn\", knn),\n])\nmodel_pipeline.fit(X_train,y_train)\npredicted = model_pipeline.predict(X_test)\nscore = model_pipeline.score(X_test,y_test)\nmodel_pipeline_score = np.mean(score)\nmodel_pipeline_score\n\n\nCode\nprint(classification_report(y_test, predicted)) \n\n\n\ndeep learining\n\nCode\nfrom dis import dis\n\n# cleaned = df.drop(['obj_ID','alpha','delta','run_ID','rerun_ID','cam_col','field_ID','fiber_ID'], axis = 1)\ncleaned = df.drop(['u','r','i','z','obj_ID','spec_obj_ID','MJD'], axis = 1)\n\n# cleaned = df.drop(df.filter(regex='ID$').columns, axis=1)\n# drop the date column\n# cleaned = cleaned.drop([\"MJD\",\"plate\",\"cam_col\"], axis=1)\ncleaned=cleaned.drop(79543)\n# make the X and y varialbes\nX = cleaned.drop('class', axis=1)\ny = cleaned['class']\ndisplay(cleaned)\n\n\nsm = SMOTE(random_state=42)\nX, y = sm.fit_resample(X, y)\n\n\nX_train, X_test, y_train, y_test = train_test_split(X, y, train_size=0.7, random_state=123)\n\nfrom tensorflow.keras import utils\n# y = utils.to_categorical(y)\n# y_train = utils.to_categorical(y_train)\n# y_test = utils.to_categorical(y_test)\n\n\nCode\n\n# define the keras model\nmodel = Sequential()\nmodel.add(Dense(units=64, input_dim=20, activation=\"tanh\"))\nmodel.add(Dense(units=64,  activation=\"tanh\"))\nmodel.add(Dense(units=32,  activation=\"tanh\"))\nmodel.add(Dense(units=3, activation='softmax'))\n\n# compile the keras model\nmodel.compile(\n    loss='sparse_categorical_crossentropy', \n    optimizer=\"rmsprop\",\n    metrics='accuracy'\n)\n# fit the model\n# model.fit(X, y, epochs=20, validation_split=0.2)\nmodel_pipeline = Pipeline(steps=[\n  (\"preprocessor\", preprocessor),\n  (\"model\", model),\n])\nm1=model_pipeline.fit(X_train,y_train, model__epochs=20, model__validation_split=0.2,model__batch_size=32,)\n\n\nCode\npredicted=m1.predict(X_test)\ny_classes = predicted.argmax(axis=-1)\n\n# model_pipeline.transform(X_test)\nprint(classification_report(y_test, y_classes)) \n\n\nCode\n# define loss function\nloss = 'accuracy'\n\n# create 10 fold CV object\nkfold = KFold(n_splits=5, random_state=123, shuffle=True)\n\n# fit model with 10-fold CV\nresults = cross_val_score(model_pipeline, X_train, y_train, cv=kfold, scoring=loss)\nresults\n\n\nCode\n# hyper_grid = {'n_neighbors': range(2, 26)}\n# grid_search = GridSearchCV(knn, hyper_grid, cv=kfold, scoring=loss)\n# results = grid_search.fit(X_train, y_train)\n\n\n\nk-means clustring | trash\n\nCode\nmodel = sklearn.cluster.KMeans(n_clusters=3,random_state=123)\nmodel\n\n\nCode\ngalaxy = cleaned[cleaned[\"class\"]==0].drop(\"class\",axis=1)\ngalaxy_centers = map(lambda a: a/galaxy.shape[0],galaxy.sum())\ngalaxy_centers= np.array(list(galaxy_centers))\ngalaxy_centers\n\n\nSTAR = cleaned[cleaned[\"class\"]==1].drop(\"class\",axis=1)\nSTAR_centers = map(lambda a: a/STAR.shape[0],STAR.sum())\nSTAR_centers= np.array(list(STAR_centers))\nSTAR_centers\n\n\n\nQSO = cleaned[cleaned[\"class\"]==2].drop(\"class\",axis=1)\nQSO_centers = map(lambda a: a/QSO.shape[0],QSO.sum())\nQSO_centers= np.array(list(QSO_centers))\nQSO_centers\n\n\n\nCode\nm1=model.fit([galaxy_centers,STAR_centers,QSO_centers])\nm1.labels_\n\n\nCode\npred= m1.predict(X_test)\nprint(classification_report(y_test, pred))"
  },
  {
    "objectID": "Decision_Tree_automl.html",
    "href": "Decision_Tree_automl.html",
    "title": "Decision Tree",
    "section": "",
    "text": "Code\n!pip install TPOT\n\n\nCode\nfrom tpot import TPOTClassifier\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import LabelEncoder\nfrom tpot.config import classifier_config_dict\n\n\nCode\nX_train, X_test, y_train, y_test = train_test_split(digits.data, digits.target,\n                                                    train_size=0.75, test_size=0.25, random_state=42\n\nDonâ€™t include: * plate ID, identifies each plate in SDSS * Unique ID used for optical spectroscopic objects (this means that 2 different observations with the same spec_obj_ID must share the output class) * Field number to identify each field * Camera column to identify the scanline within the run * Rerun Number to specify how the image was processed * Run Number used to identify the specific scan * Object Identifier, the unique value that identifies the object in the image catalog used by the CAS * MJD = Modified Julian Date, used to indicate when a given piece of SDSS data was taken\n\nCode\ndf = pd.read_csv(\"star_classification.csv\")\n#just manually remove all ID-type columns, and class\nfeatures = [\n 'alpha',\n 'delta',\n 'u',\n 'g',\n 'r',\n 'i',\n 'z',\n 'redshift']\nX = df[features]\ny = df['class']\nle = LabelEncoder()\ny = le.fit_transform(y)\n\n\nCode\ntpot_config = {\n    'sklearn.tree.DecisionTreeClassifier': {\n        'criterion': [\"gini\", \"entropy\"],\n        'max_depth': range(1, 11),\n        'min_samples_split': range(2, 21),\n        'min_samples_leaf': range(1, 21)\n    }\n}\n\n\nCode\ntpot_config['tpot.builtins.FeatureSetSelector'] = {\n    'subset_list': ['https://raw.githubusercontent.com/EpistasisLab/tpot/master/tests/subset_test.csv'],\n    'sel_subset': [0,1] # select only one feature set, a list of index of subset in the list above\n    #'sel_subset': list(combinations(range(3), 2)) # select two feature sets\n}\n\n\nCode\nX_train, X_test, y_train, y_test = train_test_split(X, y,\n                                                    train_size=0.75, test_size=0.25, random_state=42)\n\n\nCode\ntpot = TPOTClassifier(config_dict=tpot_config, generations=5, population_size=50, verbosity=2, random_state=42,max_time_mins = 5)\n\n\n\nCode\ntpot.fit(X_train, y_train)\n\n\n\n\n\n\n5.01 minutes have elapsed. TPOT will close down.\nTPOT closed during evaluation in one generation.\nWARNING: TPOT may not provide a good pipeline if TPOT is stopped/interrupted in a early generation.\n\n\nTPOT closed prematurely. Will use the current best pipeline.\n\nBest pipeline: DecisionTreeClassifier(DecisionTreeClassifier(input_matrix, criterion=entropy, max_depth=10, min_samples_leaf=19, min_samples_split=18), criterion=entropy, max_depth=3, min_samples_leaf=5, min_samples_split=10)\n\n\nTPOTClassifier(config_dict={'sklearn.tree.DecisionTreeClassifier': {'criterion': ['gini',\n                                                                                  'entropy'],\n                                                                    'max_depth': range(1, 11),\n                                                                    'min_samples_leaf': range(1, 21),\n                                                                    'min_samples_split': range(2, 21)},\n                            'tpot.builtins.FeatureSetSelector': {'sel_subset': [0,\n                                                                                1],\n                                                                 'subset_list': ['https://raw.githubusercontent.com/EpistasisLab/tpot/master/tests/subset_test.csv']}},\n               generations=5, max_time_mins=5, population_size=50,\n               random_state=42, verbosity=2)\n\n\n\n\nCode\nprint(tpot.score(X_test, y_test))\n\n\n0.97568\n\n\n\nCode\ntpot.export(\"dt_pipeline.py\")\n\n\nCode\n#run pipeline\n\n\nCode\nclf = DecisionTreeClassifier(criterion=\"entropy\", max_depth=3, min_samples_leaf=5, min_samples_split=10)\n\n\nCode\nclf.fit(X_train, y_train)\ny_pred = clf.predict(X_test)\n\n\n\nCode\nprint(\"Accuracy score\", sklearn.metrics.accuracy_score(y_test, y_pred))\n\n\nAccuracy score 0.95088\n\n\n\n\nCode\nprint(classification_report(y_test, y_pred))\n\n\n              precision    recall  f1-score   support\n\n           0       0.94      0.98      0.96     14895\n           1       0.95      0.79      0.86      4769\n           2       1.00      1.00      1.00      5336\n\n    accuracy                           0.95     25000\n   macro avg       0.96      0.93      0.94     25000\nweighted avg       0.95      0.95      0.95     25000\n\n\n\n\n\nCode\nmatrix = confusion_matrix(y_pred, y_test)\nmatrix = matrix / matrix.astype(np.float).sum(axis=1)\ncm = sns.heatmap(matrix, square=True, annot=True, cbar=False,\n            xticklabels=['GALAXY', 'STAR', 'QSO'], yticklabels=['GALAXY', 'STAR', 'QSO'])\nplt.xlabel('Truth')\nplt.ylabel('Predicted')\nplt.title(\"\")\nplt.show()\n\n\n/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:2: DeprecationWarning: `np.float` is a deprecated alias for the builtin `float`. To silence this warning, use `float` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.float64` here.\nDeprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n  \n\n\n\n\n\n\nCode\n#initialize shadow tree\nsk_dtree = ShadowSKDTree(clf, X, y, features, \"class\", ['GALAXY', 'STAR', 'QSO'])\n\n\n\nCode\ntrees.viz_leaf_samples(sk_dtree)\n\n\n[WARNING] [2022-08-09 23:43:03,900:matplotlib.font_manager] findfont: Font family ['Arial'] not found. Falling back to DejaVu Sans.\n\n\n\n\n\n\n\nCode\ntrees.dtreeviz(sk_dtree)\n\n\n[WARNING] [2022-08-09 23:43:50,124:matplotlib.font_manager] findfont: Font family ['Arial'] not found. Falling back to DejaVu Sans.\n[WARNING] [2022-08-09 23:43:50,353:matplotlib.font_manager] findfont: Font family ['Arial'] not found. Falling back to DejaVu Sans.\n[WARNING] [2022-08-09 23:43:50,369:matplotlib.font_manager] findfont: Font family ['Arial'] not found. Falling back to DejaVu Sans.\n\n\n/usr/local/lib/python3.7/dist-packages/numpy/core/fromnumeric.py:3208: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n  return asarray(a).size\n/usr/local/lib/python3.7/dist-packages/matplotlib/cbook/__init__.py:1376: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n  X = np.atleast_1d(X.T if isinstance(X, np.ndarray) else np.asarray(X))\n\n\n[WARNING] [2022-08-09 23:43:51,615:matplotlib.font_manager] findfont: Font family ['Arial'] not found. Falling back to DejaVu Sans.\n\n\n\n\n\n\n\nCode\ntrees.dtreeviz(sk_dtree, fancy=False)\n\n\n\n\n\n\n\nCode\ntrees.dtreeviz(sk_dtree, show_just_path=True, X = X.iloc[10])\n\n\n/usr/local/lib/python3.7/dist-packages/numpy/core/fromnumeric.py:3208: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n  return asarray(a).size\n/usr/local/lib/python3.7/dist-packages/matplotlib/cbook/__init__.py:1376: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n  X = np.atleast_1d(X.T if isinstance(X, np.ndarray) else np.asarray(X))\n\n\n\n\n\n\n\nCode\ntrees.viz_leaf_criterion(clf)\n\n\n\n\n\n\n\nCode\ntrees.describe_node_sample(sk_dtree, node_id=10)\n\n\n\n\n  \n    \n      \n\n\n  \n    \n      \n      alpha\n      delta\n      u\n      g\n      r\n      i\n      z\n      redshift\n    \n  \n  \n    \n      count\n      51935.000000\n      51935.000000\n      51935.000000\n      51935.000000\n      51935.000000\n      51935.000000\n      51935.000000\n      51935.000000\n    \n    \n      mean\n      181.711450\n      24.074552\n      22.346448\n      20.536128\n      19.210662\n      18.515720\n      18.135271\n      0.355549\n    \n    \n      std\n      91.416751\n      19.351312\n      2.327826\n      2.027503\n      1.723310\n      1.536114\n      1.502448\n      0.203380\n    \n    \n      min\n      0.005528\n      -12.364701\n      13.897990\n      12.679020\n      11.746640\n      11.299560\n      10.897380\n      0.004285\n    \n    \n      25%\n      134.238282\n      6.283251\n      20.344025\n      18.611765\n      17.660465\n      17.226380\n      16.926995\n      0.145027\n    \n    \n      50%\n      187.270126\n      22.799320\n      22.584700\n      21.159340\n      19.592490\n      18.855040\n      18.436680\n      0.387110\n    \n    \n      75%\n      231.975632\n      38.773742\n      24.060300\n      22.144885\n      20.597755\n      19.655370\n      19.214630\n      0.534749\n    \n    \n      max\n      359.994125\n      74.459854\n      29.325650\n      29.862580\n      29.571860\n      29.889210\n      29.383740\n      0.685050\n    \n  \n\n\n      \n        \n  \n    \n    \n  \n      \n      \n  \n\n      \n    \n  \n  \n\n\n\n\nCode\npred_path = X.iloc[10]\npred_path\n\n\nalpha       328.092076\ndelta        18.220310\nu            25.771630\ng            22.520420\nr            20.638840\ni            19.780710\nz            19.057650\nredshift      0.459596\nName: 10, dtype: float64\n\n\n\n\nCode\nprint(trees.explain_prediction_path(clf, pred_path, feature_names=features, explanation_type=\"plain_english\"))\n\n\n0.0 <= redshift  < 0.69\n\n\n\n\n\nCode\ntrees.explain_prediction_path(clf, pred_path, feature_names=features, explanation_type=\"sklearn_default\")\n\n\n<matplotlib.axes._subplots.AxesSubplot at 0x7f5a40719c90>"
  }
]